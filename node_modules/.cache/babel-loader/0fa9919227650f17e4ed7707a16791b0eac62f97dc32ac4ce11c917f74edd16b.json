{"ast":null,"code":"/**\n * @license\n * Copyright 2022 Google LLC.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { LRN } from '@tensorflow/tfjs-core';\nimport { LRNProgram, LRNSharedProgram } from '../lrn_webgpu';\nexport function lrn(args) {\n  const {\n    inputs,\n    backend,\n    attrs\n  } = args;\n  const {\n    x\n  } = inputs;\n  const {\n    depthRadius,\n    bias,\n    alpha,\n    beta\n  } = attrs;\n  // When the adjacent channels is less than or equal to 16, which could cover\n  // most cases, we use shared memory version to get better performance.\n  // The theoretical adjacent channels may be very large, but the shared memory\n  // size of hardware is limited, so we use the naive version when the adjacent\n  // channels is large.\n  let program;\n  if (depthRadius > 16) {\n    program = new LRNProgram(x.shape);\n  } else {\n    program = new LRNSharedProgram(x.shape, depthRadius);\n  }\n  const uniformData = [{\n    type: 'int32',\n    data: [depthRadius]\n  }, {\n    type: 'float32',\n    data: [bias]\n  }, {\n    type: 'float32',\n    data: [alpha]\n  }, {\n    type: 'float32',\n    data: [beta]\n  }];\n  const res = backend.runWebGPUProgram(program, [x], x.dtype, uniformData);\n  return res;\n}\nexport const lrnConfig = {\n  kernelName: LRN,\n  backendName: 'webgpu',\n  kernelFunc: lrn\n};","map":{"version":3,"names":["LRN","LRNProgram","LRNSharedProgram","lrn","args","inputs","backend","attrs","x","depthRadius","bias","alpha","beta","program","shape","uniformData","type","data","res","runWebGPUProgram","dtype","lrnConfig","kernelName","backendName","kernelFunc"],"sources":["/Users/jesvinblazegmail.com/PycharmProjects/tfjs-backend-webgpu/src/kernels/LRN.ts"],"sourcesContent":["/**\n * @license\n * Copyright 2022 Google LLC.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {KernelConfig, KernelFunc, LRN, LRNAttrs, LRNInputs, TensorInfo} from '@tensorflow/tfjs-core';\n\nimport {WebGPUBackend} from '../backend_webgpu';\nimport {LRNProgram, LRNSharedProgram} from '../lrn_webgpu';\n\nexport function lrn(\n    args: {inputs: LRNInputs, backend: WebGPUBackend, attrs: LRNAttrs}):\n    TensorInfo {\n  const {inputs, backend, attrs} = args;\n  const {x} = inputs;\n  const {depthRadius, bias, alpha, beta} = attrs;\n\n  // When the adjacent channels is less than or equal to 16, which could cover\n  // most cases, we use shared memory version to get better performance.\n  // The theoretical adjacent channels may be very large, but the shared memory\n  // size of hardware is limited, so we use the naive version when the adjacent\n  // channels is large.\n  let program: LRNProgram|LRNSharedProgram;\n  if (depthRadius > 16) {\n    program = new LRNProgram(x.shape);\n  } else {\n    program = new LRNSharedProgram(x.shape, depthRadius);\n  }\n  const uniformData = [\n    {type: 'int32', data: [depthRadius]}, {type: 'float32', data: [bias]},\n    {type: 'float32', data: [alpha]}, {type: 'float32', data: [beta]}\n  ];\n  const res = backend.runWebGPUProgram(program, [x], x.dtype, uniformData);\n\n  return res;\n}\n\nexport const lrnConfig: KernelConfig = {\n  kernelName: LRN,\n  backendName: 'webgpu',\n  kernelFunc: lrn as unknown as KernelFunc\n};\n"],"mappings":"AAAA;;;;;;;;;;;;;;;;AAiBA,SAAkCA,GAAG,QAAwC,uBAAuB;AAGpG,SAAQC,UAAU,EAAEC,gBAAgB,QAAO,eAAe;AAE1D,OAAM,SAAUC,GAAGA,CACfC,IAAkE;EAEpE,MAAM;IAACC,MAAM;IAAEC,OAAO;IAAEC;EAAK,CAAC,GAAGH,IAAI;EACrC,MAAM;IAACI;EAAC,CAAC,GAAGH,MAAM;EAClB,MAAM;IAACI,WAAW;IAAEC,IAAI;IAAEC,KAAK;IAAEC;EAAI,CAAC,GAAGL,KAAK;EAE9C;EACA;EACA;EACA;EACA;EACA,IAAIM,OAAoC;EACxC,IAAIJ,WAAW,GAAG,EAAE,EAAE;IACpBI,OAAO,GAAG,IAAIZ,UAAU,CAACO,CAAC,CAACM,KAAK,CAAC;GAClC,MAAM;IACLD,OAAO,GAAG,IAAIX,gBAAgB,CAACM,CAAC,CAACM,KAAK,EAAEL,WAAW,CAAC;;EAEtD,MAAMM,WAAW,GAAG,CAClB;IAACC,IAAI,EAAE,OAAO;IAAEC,IAAI,EAAE,CAACR,WAAW;EAAC,CAAC,EAAE;IAACO,IAAI,EAAE,SAAS;IAAEC,IAAI,EAAE,CAACP,IAAI;EAAC,CAAC,EACrE;IAACM,IAAI,EAAE,SAAS;IAAEC,IAAI,EAAE,CAACN,KAAK;EAAC,CAAC,EAAE;IAACK,IAAI,EAAE,SAAS;IAAEC,IAAI,EAAE,CAACL,IAAI;EAAC,CAAC,CAClE;EACD,MAAMM,GAAG,GAAGZ,OAAO,CAACa,gBAAgB,CAACN,OAAO,EAAE,CAACL,CAAC,CAAC,EAAEA,CAAC,CAACY,KAAK,EAAEL,WAAW,CAAC;EAExE,OAAOG,GAAG;AACZ;AAEA,OAAO,MAAMG,SAAS,GAAiB;EACrCC,UAAU,EAAEtB,GAAG;EACfuB,WAAW,EAAE,QAAQ;EACrBC,UAAU,EAAErB;CACb","ignoreList":[]},"metadata":{},"sourceType":"module","externalDependencies":[]}