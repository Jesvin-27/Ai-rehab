{"ast":null,"code":"/**\n * @license\n * Copyright 2021 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { backend_util, sumOutType, util } from '@tensorflow/tfjs-core';\nimport { reshape } from '../kernels/Reshape';\nimport { transpose } from '../kernels/Transpose';\nimport { ReduceProgram } from '../reduce_webgpu';\nimport { maxImplCPU } from './shared';\nimport { prodImplCPU } from './shared';\nconst RETURN_TYPES = {\n  'mean': 'float32',\n  'all': 'bool',\n  'any': 'bool'\n};\nexport function reduce(x, axis, keepDims, reduceType, backend) {\n  const xRank = x.shape.length;\n  const toDispose = [];\n  const origAxes = util.parseAxisParam(axis, x.shape);\n  let axes = origAxes;\n  const permutedAxes = backend_util.getAxesPermutation(axes, xRank);\n  let input = x;\n  if (permutedAxes != null) {\n    input = transpose({\n      inputs: {\n        x\n      },\n      attrs: {\n        perm: permutedAxes\n      },\n      backend\n    });\n    axes = backend_util.getInnerMostAxes(axes.length, xRank);\n    toDispose.push(input);\n  }\n  backend_util.assertAxesAreInnerMostDims(reduceType, axes, xRank);\n  const [reduceOutShape, reduceShape] = backend_util.computeOutAndReduceShapes(input.shape, axes);\n  let resOutShape = reduceOutShape;\n  if (keepDims) {\n    // rather than reshape at the end, set the target shape here.\n    resOutShape = backend_util.expandShapeToKeepDim(reduceOutShape, origAxes);\n  }\n  let res;\n  if ((reduceType === 'max' || reduceType === 'prod') && backend.shouldExecuteOnCPU([input])) {\n    const xVals = backend.tensorMap.get(input.dataId).values;\n    switch (reduceType) {\n      case 'max':\n        const outValues = maxImplCPU(xVals, util.sizeFromShape(reduceShape), resOutShape, x.dtype);\n        res = backend.makeTensorInfo(resOutShape, x.dtype, outValues);\n        break;\n      case 'prod':\n        const {\n          outVals,\n          outShape,\n          outDtype\n        } = prodImplCPU(input.shape, input.dtype, xVals, axes);\n        res = backend.makeTensorInfo(outShape, outDtype, outVals);\n        break;\n      default:\n        throw new Error(`${reduceType} CPU implementation is not yet supported.`);\n    }\n  } else {\n    const inSize = util.sizeFromShape(reduceShape);\n    const xSize = util.sizeFromShape(input.shape);\n    const batchSize = xSize / inSize;\n    const reduceInfo = {\n      windowSize: inSize,\n      inSize,\n      batchSize,\n      outSize: 1\n    };\n    const dtype = RETURN_TYPES[reduceType] || sumOutType(x.dtype);\n    const uniformData = [{\n      type: 'int32',\n      data: [inSize]\n    }];\n    const program = new ReduceProgram(reduceInfo, reduceType, backend.device.limits.maxComputeWorkgroupSizeX);\n    const reduced = backend.runWebGPUProgram(program, [input], dtype, uniformData);\n    toDispose.push(reduced);\n    res = reshape({\n      inputs: {\n        x: reduced\n      },\n      attrs: {\n        shape: resOutShape\n      },\n      backend\n    });\n  }\n  toDispose.forEach(t => backend.disposeData(t.dataId));\n  return res;\n}","map":{"version":3,"names":["backend_util","sumOutType","util","reshape","transpose","ReduceProgram","maxImplCPU","prodImplCPU","RETURN_TYPES","reduce","x","axis","keepDims","reduceType","backend","xRank","shape","length","toDispose","origAxes","parseAxisParam","axes","permutedAxes","getAxesPermutation","input","inputs","attrs","perm","getInnerMostAxes","push","assertAxesAreInnerMostDims","reduceOutShape","reduceShape","computeOutAndReduceShapes","resOutShape","expandShapeToKeepDim","res","shouldExecuteOnCPU","xVals","tensorMap","get","dataId","values","outValues","sizeFromShape","dtype","makeTensorInfo","outVals","outShape","outDtype","Error","inSize","xSize","batchSize","reduceInfo","windowSize","outSize","uniformData","type","data","program","device","limits","maxComputeWorkgroupSizeX","reduced","runWebGPUProgram","forEach","t","disposeData"],"sources":["/Users/jesvinblazegmail.com/PycharmProjects/tfjs-backend-webgpu/src/kernel_utils/reduce.ts"],"sourcesContent":["/**\n * @license\n * Copyright 2021 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {backend_util, DataType, sumOutType, TensorInfo, TypedArray, util} from '@tensorflow/tfjs-core';\n\nimport {WebGPUBackend} from '../backend_webgpu';\nimport {reshape} from '../kernels/Reshape';\nimport {transpose} from '../kernels/Transpose';\nimport {ReduceProgram} from '../reduce_webgpu';\n\nimport {maxImplCPU} from './shared';\nimport {prodImplCPU} from './shared';\n\ntype ReduceTypes = 'all'|'any'|'max'|'mean'|'min'|'prod'|'sum';\nconst RETURN_TYPES: {[key in ReduceTypes]?: DataType} = {\n  'mean': 'float32',\n  'all': 'bool',\n  'any': 'bool',\n};\n\nexport function reduce(\n    x: TensorInfo, axis: number|number[], keepDims: boolean,\n    reduceType: ReduceTypes, backend: WebGPUBackend): TensorInfo {\n  const xRank = x.shape.length;\n  const toDispose = [];\n\n  const origAxes = util.parseAxisParam(axis, x.shape);\n  let axes = origAxes;\n  const permutedAxes = backend_util.getAxesPermutation(axes, xRank);\n\n  let input = x;\n  if (permutedAxes != null) {\n    input = transpose({inputs: {x}, attrs: {perm: permutedAxes}, backend});\n    axes = backend_util.getInnerMostAxes(axes.length, xRank);\n    toDispose.push(input);\n  }\n\n  backend_util.assertAxesAreInnerMostDims(reduceType, axes, xRank);\n\n  const [reduceOutShape, reduceShape] =\n      backend_util.computeOutAndReduceShapes(input.shape, axes);\n  let resOutShape = reduceOutShape;\n  if (keepDims) {\n    // rather than reshape at the end, set the target shape here.\n    resOutShape = backend_util.expandShapeToKeepDim(reduceOutShape, origAxes);\n  }\n\n  let res;\n  if ((reduceType === 'max' || reduceType === 'prod') &&\n      backend.shouldExecuteOnCPU([input])) {\n    const xVals = backend.tensorMap.get(input.dataId).values as TypedArray;\n    switch (reduceType) {\n      case 'max':\n        const outValues = maxImplCPU(\n            xVals, util.sizeFromShape(reduceShape), resOutShape, x.dtype);\n        res = backend.makeTensorInfo(resOutShape, x.dtype, outValues);\n        break;\n      case 'prod':\n        const {outVals, outShape, outDtype} =\n            prodImplCPU(input.shape, input.dtype, xVals, axes);\n        res = backend.makeTensorInfo(outShape, outDtype, outVals);\n        break;\n      default:\n        throw new Error(\n            `${reduceType} CPU implementation is not yet supported.`);\n    }\n  } else {\n    const inSize = util.sizeFromShape(reduceShape);\n    const xSize = util.sizeFromShape(input.shape);\n    const batchSize = xSize / inSize;\n\n    const reduceInfo = {windowSize: inSize, inSize, batchSize, outSize: 1};\n    const dtype = RETURN_TYPES[reduceType] || sumOutType(x.dtype);\n    const uniformData = [\n      {type: 'int32', data: [inSize]},\n    ];\n    const program = new ReduceProgram(\n        reduceInfo, reduceType, backend.device.limits.maxComputeWorkgroupSizeX);\n    const reduced =\n        backend.runWebGPUProgram(program, [input], dtype, uniformData);\n    toDispose.push(reduced);\n\n    res = reshape({inputs: {x: reduced}, attrs: {shape: resOutShape}, backend});\n  }\n\n  toDispose.forEach(t => backend.disposeData(t.dataId));\n\n  return res;\n}\n"],"mappings":"AAAA;;;;;;;;;;;;;;;;AAiBA,SAAQA,YAAY,EAAYC,UAAU,EAA0BC,IAAI,QAAO,uBAAuB;AAGtG,SAAQC,OAAO,QAAO,oBAAoB;AAC1C,SAAQC,SAAS,QAAO,sBAAsB;AAC9C,SAAQC,aAAa,QAAO,kBAAkB;AAE9C,SAAQC,UAAU,QAAO,UAAU;AACnC,SAAQC,WAAW,QAAO,UAAU;AAGpC,MAAMC,YAAY,GAAsC;EACtD,MAAM,EAAE,SAAS;EACjB,KAAK,EAAE,MAAM;EACb,KAAK,EAAE;CACR;AAED,OAAM,SAAUC,MAAMA,CAClBC,CAAa,EAAEC,IAAqB,EAAEC,QAAiB,EACvDC,UAAuB,EAAEC,OAAsB;EACjD,MAAMC,KAAK,GAAGL,CAAC,CAACM,KAAK,CAACC,MAAM;EAC5B,MAAMC,SAAS,GAAG,EAAE;EAEpB,MAAMC,QAAQ,GAAGjB,IAAI,CAACkB,cAAc,CAACT,IAAI,EAAED,CAAC,CAACM,KAAK,CAAC;EACnD,IAAIK,IAAI,GAAGF,QAAQ;EACnB,MAAMG,YAAY,GAAGtB,YAAY,CAACuB,kBAAkB,CAACF,IAAI,EAAEN,KAAK,CAAC;EAEjE,IAAIS,KAAK,GAAGd,CAAC;EACb,IAAIY,YAAY,IAAI,IAAI,EAAE;IACxBE,KAAK,GAAGpB,SAAS,CAAC;MAACqB,MAAM,EAAE;QAACf;MAAC,CAAC;MAAEgB,KAAK,EAAE;QAACC,IAAI,EAAEL;MAAY,CAAC;MAAER;IAAO,CAAC,CAAC;IACtEO,IAAI,GAAGrB,YAAY,CAAC4B,gBAAgB,CAACP,IAAI,CAACJ,MAAM,EAAEF,KAAK,CAAC;IACxDG,SAAS,CAACW,IAAI,CAACL,KAAK,CAAC;;EAGvBxB,YAAY,CAAC8B,0BAA0B,CAACjB,UAAU,EAAEQ,IAAI,EAAEN,KAAK,CAAC;EAEhE,MAAM,CAACgB,cAAc,EAAEC,WAAW,CAAC,GAC/BhC,YAAY,CAACiC,yBAAyB,CAACT,KAAK,CAACR,KAAK,EAAEK,IAAI,CAAC;EAC7D,IAAIa,WAAW,GAAGH,cAAc;EAChC,IAAInB,QAAQ,EAAE;IACZ;IACAsB,WAAW,GAAGlC,YAAY,CAACmC,oBAAoB,CAACJ,cAAc,EAAEZ,QAAQ,CAAC;;EAG3E,IAAIiB,GAAG;EACP,IAAI,CAACvB,UAAU,KAAK,KAAK,IAAIA,UAAU,KAAK,MAAM,KAC9CC,OAAO,CAACuB,kBAAkB,CAAC,CAACb,KAAK,CAAC,CAAC,EAAE;IACvC,MAAMc,KAAK,GAAGxB,OAAO,CAACyB,SAAS,CAACC,GAAG,CAAChB,KAAK,CAACiB,MAAM,CAAC,CAACC,MAAoB;IACtE,QAAQ7B,UAAU;MAChB,KAAK,KAAK;QACR,MAAM8B,SAAS,GAAGrC,UAAU,CACxBgC,KAAK,EAAEpC,IAAI,CAAC0C,aAAa,CAACZ,WAAW,CAAC,EAAEE,WAAW,EAAExB,CAAC,CAACmC,KAAK,CAAC;QACjET,GAAG,GAAGtB,OAAO,CAACgC,cAAc,CAACZ,WAAW,EAAExB,CAAC,CAACmC,KAAK,EAAEF,SAAS,CAAC;QAC7D;MACF,KAAK,MAAM;QACT,MAAM;UAACI,OAAO;UAAEC,QAAQ;UAAEC;QAAQ,CAAC,GAC/B1C,WAAW,CAACiB,KAAK,CAACR,KAAK,EAAEQ,KAAK,CAACqB,KAAK,EAAEP,KAAK,EAAEjB,IAAI,CAAC;QACtDe,GAAG,GAAGtB,OAAO,CAACgC,cAAc,CAACE,QAAQ,EAAEC,QAAQ,EAAEF,OAAO,CAAC;QACzD;MACF;QACE,MAAM,IAAIG,KAAK,CACX,GAAGrC,UAAU,2CAA2C,CAAC;;GAElE,MAAM;IACL,MAAMsC,MAAM,GAAGjD,IAAI,CAAC0C,aAAa,CAACZ,WAAW,CAAC;IAC9C,MAAMoB,KAAK,GAAGlD,IAAI,CAAC0C,aAAa,CAACpB,KAAK,CAACR,KAAK,CAAC;IAC7C,MAAMqC,SAAS,GAAGD,KAAK,GAAGD,MAAM;IAEhC,MAAMG,UAAU,GAAG;MAACC,UAAU,EAAEJ,MAAM;MAAEA,MAAM;MAAEE,SAAS;MAAEG,OAAO,EAAE;IAAC,CAAC;IACtE,MAAMX,KAAK,GAAGrC,YAAY,CAACK,UAAU,CAAC,IAAIZ,UAAU,CAACS,CAAC,CAACmC,KAAK,CAAC;IAC7D,MAAMY,WAAW,GAAG,CAClB;MAACC,IAAI,EAAE,OAAO;MAAEC,IAAI,EAAE,CAACR,MAAM;IAAC,CAAC,CAChC;IACD,MAAMS,OAAO,GAAG,IAAIvD,aAAa,CAC7BiD,UAAU,EAAEzC,UAAU,EAAEC,OAAO,CAAC+C,MAAM,CAACC,MAAM,CAACC,wBAAwB,CAAC;IAC3E,MAAMC,OAAO,GACTlD,OAAO,CAACmD,gBAAgB,CAACL,OAAO,EAAE,CAACpC,KAAK,CAAC,EAAEqB,KAAK,EAAEY,WAAW,CAAC;IAClEvC,SAAS,CAACW,IAAI,CAACmC,OAAO,CAAC;IAEvB5B,GAAG,GAAGjC,OAAO,CAAC;MAACsB,MAAM,EAAE;QAACf,CAAC,EAAEsD;MAAO,CAAC;MAAEtC,KAAK,EAAE;QAACV,KAAK,EAAEkB;MAAW,CAAC;MAAEpB;IAAO,CAAC,CAAC;;EAG7EI,SAAS,CAACgD,OAAO,CAACC,CAAC,IAAIrD,OAAO,CAACsD,WAAW,CAACD,CAAC,CAAC1B,MAAM,CAAC,CAAC;EAErD,OAAOL,GAAG;AACZ","ignoreList":[]},"metadata":{},"sourceType":"module","externalDependencies":[]}