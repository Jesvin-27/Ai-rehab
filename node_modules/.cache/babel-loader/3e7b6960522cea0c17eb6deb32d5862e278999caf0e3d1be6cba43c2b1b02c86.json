{"ast":null,"code":"/**\n * @license\n * Copyright 2021 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { broadcast_util, env, util } from '@tensorflow/tfjs-core';\nimport { MatMulPackedProgram } from '../matmul_packed_webgpu';\nimport { MatMulReduceProgram } from '../matmul_reduce_webgpu';\nimport { MatMulSmallOutputSizeProgram } from '../matmul_small_output_size_webgpu';\nimport { BiasActivationProgram, MatMulSplitKProgram } from '../matmul_splitK_webgpu';\nimport { MatMulProgramType } from '../webgpu_util';\nimport { fill } from './Fill';\nimport { reshape } from './Reshape';\nexport function batchMatMulImpl({\n  a,\n  b,\n  transposeA,\n  transposeB,\n  backend,\n  bias = null,\n  preluActivationWeights = null,\n  leakyreluAlpha = 0,\n  activation = null\n}) {\n  const aRank = a.shape.length;\n  const bRank = b.shape.length;\n  const innerShapeA = transposeA ? a.shape[aRank - 2] : a.shape[aRank - 1];\n  const innerShapeB = transposeB ? b.shape[bRank - 1] : b.shape[bRank - 2];\n  const outerShapeA = transposeA ? a.shape[aRank - 1] : a.shape[aRank - 2];\n  const outerShapeB = transposeB ? b.shape[bRank - 2] : b.shape[bRank - 1];\n  const outerDimsA = a.shape.slice(0, -2);\n  const outerDimsB = b.shape.slice(0, -2);\n  const batchDimA = util.sizeFromShape(outerDimsA);\n  const batchDimB = util.sizeFromShape(outerDimsB);\n  const outShapeOuterDims = broadcast_util.assertAndGetBroadcastShape(a.shape.slice(0, -2), b.shape.slice(0, -2));\n  const outShape = outShapeOuterDims.concat([outerShapeA, outerShapeB]);\n  util.assert(innerShapeA === innerShapeB, () => `Error in matMul: inner shapes (${innerShapeA}) and (` + `${innerShapeB}) of Tensors with shapes ${a.shape} and ` + `${b.shape} and transposeA=${transposeA}` + ` and transposeB=${transposeB} must match.`);\n  const a3dShape = transposeA ? [batchDimA, innerShapeA, outerShapeA] : [batchDimA, outerShapeA, innerShapeA];\n  const b3dShape = transposeB ? [batchDimB, outerShapeB, innerShapeB] : [batchDimB, innerShapeB, outerShapeB];\n  // The rest of the implementation is designed to operate on rank-3 tensors\n  const a3d = reshape({\n    inputs: {\n      x: a\n    },\n    backend,\n    attrs: {\n      shape: a3dShape\n    }\n  });\n  const b3d = reshape({\n    inputs: {\n      x: b\n    },\n    backend,\n    attrs: {\n      shape: b3dShape\n    }\n  });\n  const intermediates = [a3d, b3d];\n  const batchDim = Math.max(batchDimA, batchDimB);\n  const inputs = [a3d, b3d];\n  const dimensions = [{\n    type: 'int32',\n    data: [outerShapeA]\n  }, {\n    type: 'int32',\n    data: [outerShapeB]\n  }, {\n    type: 'int32',\n    data: [innerShapeA]\n  }];\n  let program;\n  let out;\n  const outputShape = [batchDim, outerShapeA, outerShapeB];\n  let matmulProgramType = env().get('WEBGPU_MATMUL_PROGRAM_TYPE');\n  if (matmulProgramType < 0) {\n    // Usually increasing workgroups is a good way to gain more performance for\n    // few workgroups by tiling 32x32 (default matmul algorithm). Currently,\n    // there are three ways to increase workgroups. 1) MatMulReduceProgram,\n    // which is used only when the output size is very small (128 for now). 2)\n    // MatMulSplitKProgram, increasing workgroups by spliting K. 3)\n    // MatMulSmallOutputSizeProgram, increasing workgroups by small tile size.\n    // For different devices, the minimum optimal workgroups may be different.\n    // So here we set a |thresholdToIncreaseWorkgroups| to indicate whether we\n    // need to increase workgroups. And the literal number is an empirical\n    // value.\n    const thresholdFlagValue = env().getNumber('WEBGPU_THRESHOLD_TO_INCREASE_WORKGROUPS_FOR_MATMUL');\n    const thresholdToIncreaseWorkgroups = thresholdFlagValue > 0 ? thresholdFlagValue : backend.thresholdToIncreaseWorkgroups;\n    const workgroupsBy32x32 = batchDim * Math.ceil(outerShapeA / 32) * Math.ceil(outerShapeB / 32);\n    const hasFewWorkgroups = workgroupsBy32x32 <= thresholdToIncreaseWorkgroups || outerShapeA <= 8 && workgroupsBy32x32 <= thresholdToIncreaseWorkgroups * 2;\n    if (hasFewWorkgroups) {\n      if (batchDim * outerShapeA * outerShapeB <= 128) {\n        matmulProgramType = MatMulProgramType.MatMulReduceProgram;\n      } else if (batchDim === 1 && innerShapeB >= 2000) {\n        matmulProgramType = MatMulProgramType.MatMulSplitKProgram;\n      } else {\n        matmulProgramType = MatMulProgramType.MatMulSmallOutputSizeProgram;\n      }\n    } else {\n      matmulProgramType = MatMulProgramType.MatMulPackedProgram;\n    }\n  }\n  switch (matmulProgramType) {\n    case MatMulProgramType.MatMulReduceProgram:\n      program = new MatMulReduceProgram(outputShape, transposeA, transposeB, bias, activation, preluActivationWeights);\n      break;\n    case MatMulProgramType.MatMulSplitKProgram:\n      {\n        // The output buffer must be initailzed to zero before using since we\n        // use atomicAdd in MatMulSplitKProgram.\n        out = fill({\n          backend,\n          attrs: {\n            shape: outputShape,\n            value: 0,\n            dtype: a.dtype\n          }\n        });\n        program = new MatMulSplitKProgram(outputShape, innerShapeB, transposeA, transposeB);\n        if (bias || activation) {\n          out = backend.runWebGPUProgram(program, inputs, a.dtype, dimensions, out);\n          const biasActivationProgram = new BiasActivationProgram(out.shape, bias, activation, preluActivationWeights);\n          let uniformData = null;\n          const activationInputs = [out];\n          if (bias) {\n            activationInputs.push(bias);\n          }\n          if (preluActivationWeights) {\n            activationInputs.push(preluActivationWeights);\n          }\n          if (activation === 'leakyrelu') {\n            uniformData = [{\n              type: 'float32',\n              data: [leakyreluAlpha]\n            }];\n            biasActivationProgram.uniforms += ' alpha : f32,';\n          }\n          const outActivated = backend.runWebGPUProgram(biasActivationProgram, activationInputs, out.dtype, uniformData);\n          intermediates.push(out);\n          const outReshaped = reshape({\n            inputs: {\n              x: outActivated\n            },\n            backend,\n            attrs: {\n              shape: outShape\n            }\n          });\n          intermediates.push(outActivated);\n          for (const i of intermediates) {\n            backend.disposeData(i.dataId);\n          }\n          return outReshaped;\n        }\n        break;\n      }\n    case MatMulProgramType.MatMulSmallOutputSizeProgram:\n      program = new MatMulSmallOutputSizeProgram(a3dShape, b3dShape, outputShape, transposeA, transposeB, bias, activation, preluActivationWeights);\n      break;\n    case MatMulProgramType.MatMulPackedProgram:\n      // Experiments show that sequential access is more friendly for Intel\n      // GPUs.\n      const sequentialAccessByThreads = backend.adapterInfo.isIntel();\n      program = new MatMulPackedProgram(a3dShape, outputShape, transposeA, transposeB, bias, activation, preluActivationWeights, sequentialAccessByThreads);\n      break;\n    default:\n      throw new Error(`Unsupported MatMulProgramType ${matmulProgramType}.`);\n  }\n  if (bias) {\n    inputs.push(bias);\n  }\n  if (preluActivationWeights) {\n    inputs.push(preluActivationWeights);\n  }\n  if (activation === 'leakyrelu') {\n    dimensions.push({\n      type: 'float32',\n      data: [leakyreluAlpha]\n    });\n    program.uniforms += ' alpha : f32,';\n  }\n  out = backend.runWebGPUProgram(program, inputs, a.dtype, dimensions, out);\n  const outReshaped = reshape({\n    inputs: {\n      x: out\n    },\n    backend,\n    attrs: {\n      shape: outShape\n    }\n  });\n  intermediates.push(out);\n  for (const i of intermediates) {\n    backend.disposeData(i.dataId);\n  }\n  return outReshaped;\n}","map":{"version":3,"names":["broadcast_util","env","util","MatMulPackedProgram","MatMulReduceProgram","MatMulSmallOutputSizeProgram","BiasActivationProgram","MatMulSplitKProgram","MatMulProgramType","fill","reshape","batchMatMulImpl","a","b","transposeA","transposeB","backend","bias","preluActivationWeights","leakyreluAlpha","activation","aRank","shape","length","bRank","innerShapeA","innerShapeB","outerShapeA","outerShapeB","outerDimsA","slice","outerDimsB","batchDimA","sizeFromShape","batchDimB","outShapeOuterDims","assertAndGetBroadcastShape","outShape","concat","assert","a3dShape","b3dShape","a3d","inputs","x","attrs","b3d","intermediates","batchDim","Math","max","dimensions","type","data","program","out","outputShape","matmulProgramType","get","thresholdFlagValue","getNumber","thresholdToIncreaseWorkgroups","workgroupsBy32x32","ceil","hasFewWorkgroups","value","dtype","runWebGPUProgram","biasActivationProgram","uniformData","activationInputs","push","uniforms","outActivated","outReshaped","i","disposeData","dataId","sequentialAccessByThreads","adapterInfo","isIntel","Error"],"sources":["/Users/jesvinblazegmail.com/PycharmProjects/tfjs-backend-webgpu/src/kernels/BatchMatMul_impl.ts"],"sourcesContent":["/**\n * @license\n * Copyright 2021 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {backend_util, broadcast_util, env, TensorInfo, util} from '@tensorflow/tfjs-core';\n\nimport {WebGPUBackend} from '../backend_webgpu';\nimport {MatMulPackedProgram} from '../matmul_packed_webgpu';\nimport {MatMulReduceProgram} from '../matmul_reduce_webgpu';\nimport {MatMulSmallOutputSizeProgram} from '../matmul_small_output_size_webgpu';\nimport {BiasActivationProgram, MatMulSplitKProgram} from '../matmul_splitK_webgpu';\nimport {WebGPUProgram} from '../webgpu_program';\nimport {MatMulProgramType} from '../webgpu_util';\n\nimport {fill} from './Fill';\nimport {reshape} from './Reshape';\n\ntype BatchMatMulConfig = {\n  a: TensorInfo,\n  b: TensorInfo,\n  transposeA: boolean,\n  transposeB: boolean,\n  backend: WebGPUBackend,\n  bias?: TensorInfo,\n  preluActivationWeights?: TensorInfo,\n  leakyreluAlpha?: number,\n  activation?: backend_util.Activation\n};\n\nexport function batchMatMulImpl({\n  a,\n  b,\n  transposeA,\n  transposeB,\n  backend,\n  bias = null,\n  preluActivationWeights = null,\n  leakyreluAlpha = 0,\n  activation = null\n}: BatchMatMulConfig): TensorInfo {\n  const aRank = a.shape.length;\n  const bRank = b.shape.length;\n\n  const innerShapeA = transposeA ? a.shape[aRank - 2] : a.shape[aRank - 1];\n  const innerShapeB = transposeB ? b.shape[bRank - 1] : b.shape[bRank - 2];\n\n  const outerShapeA = transposeA ? a.shape[aRank - 1] : a.shape[aRank - 2];\n  const outerShapeB = transposeB ? b.shape[bRank - 2] : b.shape[bRank - 1];\n\n  const outerDimsA = a.shape.slice(0, -2);\n  const outerDimsB = b.shape.slice(0, -2);\n\n  const batchDimA = util.sizeFromShape(outerDimsA);\n  const batchDimB = util.sizeFromShape(outerDimsB);\n\n  const outShapeOuterDims = broadcast_util.assertAndGetBroadcastShape(\n      a.shape.slice(0, -2), b.shape.slice(0, -2));\n  const outShape = outShapeOuterDims.concat([outerShapeA, outerShapeB]);\n\n  util.assert(\n      innerShapeA === innerShapeB,\n      () => `Error in matMul: inner shapes (${innerShapeA}) and (` +\n          `${innerShapeB}) of Tensors with shapes ${a.shape} and ` +\n          `${b.shape} and transposeA=${transposeA}` +\n          ` and transposeB=${transposeB} must match.`);\n\n  const a3dShape: [number, number, number] = transposeA ?\n      [batchDimA, innerShapeA, outerShapeA] :\n      [batchDimA, outerShapeA, innerShapeA];\n  const b3dShape: [number, number, number] = transposeB ?\n      [batchDimB, outerShapeB, innerShapeB] :\n      [batchDimB, innerShapeB, outerShapeB];\n\n  // The rest of the implementation is designed to operate on rank-3 tensors\n  const a3d = reshape({inputs: {x: a}, backend, attrs: {shape: a3dShape}});\n  const b3d = reshape({inputs: {x: b}, backend, attrs: {shape: b3dShape}});\n  const intermediates: TensorInfo[] = [a3d, b3d];\n\n  const batchDim = Math.max(batchDimA, batchDimB);\n\n  const inputs: TensorInfo[] = [a3d, b3d];\n  const dimensions = [\n    {type: 'int32', data: [outerShapeA]}, {type: 'int32', data: [outerShapeB]},\n    {type: 'int32', data: [innerShapeA]}\n  ];\n\n  let program: WebGPUProgram;\n  let out: TensorInfo;\n  const outputShape: [number, number, number] =\n      [batchDim, outerShapeA, outerShapeB];\n  let matmulProgramType = env().get('WEBGPU_MATMUL_PROGRAM_TYPE') as number;\n  if (matmulProgramType < 0) {\n    // Usually increasing workgroups is a good way to gain more performance for\n    // few workgroups by tiling 32x32 (default matmul algorithm). Currently,\n    // there are three ways to increase workgroups. 1) MatMulReduceProgram,\n    // which is used only when the output size is very small (128 for now). 2)\n    // MatMulSplitKProgram, increasing workgroups by spliting K. 3)\n    // MatMulSmallOutputSizeProgram, increasing workgroups by small tile size.\n    // For different devices, the minimum optimal workgroups may be different.\n    // So here we set a |thresholdToIncreaseWorkgroups| to indicate whether we\n    // need to increase workgroups. And the literal number is an empirical\n    // value.\n    const thresholdFlagValue =\n        env().getNumber('WEBGPU_THRESHOLD_TO_INCREASE_WORKGROUPS_FOR_MATMUL');\n    const thresholdToIncreaseWorkgroups = thresholdFlagValue > 0 ?\n        thresholdFlagValue :\n        backend.thresholdToIncreaseWorkgroups;\n    const workgroupsBy32x32 =\n        batchDim * Math.ceil(outerShapeA / 32) * Math.ceil(outerShapeB / 32);\n    const hasFewWorkgroups =\n        workgroupsBy32x32 <= thresholdToIncreaseWorkgroups ||\n        (outerShapeA <= 8 &&\n         workgroupsBy32x32 <= thresholdToIncreaseWorkgroups * 2);\n    if (hasFewWorkgroups) {\n      if (batchDim * outerShapeA * outerShapeB <= 128) {\n        matmulProgramType = MatMulProgramType.MatMulReduceProgram;\n      } else if (batchDim === 1 && innerShapeB >= 2000) {\n        matmulProgramType = MatMulProgramType.MatMulSplitKProgram;\n      } else {\n        matmulProgramType = MatMulProgramType.MatMulSmallOutputSizeProgram;\n      }\n    } else {\n      matmulProgramType = MatMulProgramType.MatMulPackedProgram;\n    }\n  }\n\n  switch (matmulProgramType) {\n    case MatMulProgramType.MatMulReduceProgram:\n      program = new MatMulReduceProgram(\n          outputShape, transposeA, transposeB, bias, activation,\n          preluActivationWeights);\n      break;\n    case MatMulProgramType.MatMulSplitKProgram: {\n      // The output buffer must be initailzed to zero before using since we\n      // use atomicAdd in MatMulSplitKProgram.\n      out = fill(\n          {backend, attrs: {shape: outputShape, value: 0, dtype: a.dtype}});\n      program = new MatMulSplitKProgram(\n          outputShape, innerShapeB, transposeA, transposeB);\n      if (bias || activation) {\n        out =\n            backend.runWebGPUProgram(program, inputs, a.dtype, dimensions, out);\n        const biasActivationProgram = new BiasActivationProgram(\n            out.shape, bias, activation, preluActivationWeights);\n        let uniformData = null;\n        const activationInputs: TensorInfo[] = [out];\n        if (bias) {\n          activationInputs.push(bias);\n        }\n        if (preluActivationWeights) {\n          activationInputs.push(preluActivationWeights);\n        }\n        if (activation === 'leakyrelu') {\n          uniformData = [{type: 'float32', data: [leakyreluAlpha]}];\n          biasActivationProgram.uniforms += ' alpha : f32,';\n        }\n        const outActivated = backend.runWebGPUProgram(\n            biasActivationProgram, activationInputs, out.dtype, uniformData);\n        intermediates.push(out);\n        const outReshaped = reshape(\n            {inputs: {x: outActivated}, backend, attrs: {shape: outShape}});\n        intermediates.push(outActivated);\n        for (const i of intermediates) {\n          backend.disposeData(i.dataId);\n        }\n        return outReshaped;\n      }\n      break;\n    }\n    case MatMulProgramType.MatMulSmallOutputSizeProgram:\n      program = new MatMulSmallOutputSizeProgram(\n          a3dShape, b3dShape, outputShape, transposeA, transposeB, bias,\n          activation, preluActivationWeights);\n      break;\n    case MatMulProgramType.MatMulPackedProgram:\n      // Experiments show that sequential access is more friendly for Intel\n      // GPUs.\n      const sequentialAccessByThreads = backend.adapterInfo.isIntel();\n      program = new MatMulPackedProgram(\n          a3dShape, outputShape, transposeA, transposeB, bias, activation,\n          preluActivationWeights, sequentialAccessByThreads);\n      break;\n    default:\n      throw new Error(`Unsupported MatMulProgramType ${matmulProgramType}.`);\n  }\n\n  if (bias) {\n    inputs.push(bias);\n  }\n  if (preluActivationWeights) {\n    inputs.push(preluActivationWeights);\n  }\n  if (activation === 'leakyrelu') {\n    dimensions.push({type: 'float32', data: [leakyreluAlpha]});\n    program.uniforms += ' alpha : f32,';\n  }\n  out = backend.runWebGPUProgram(program, inputs, a.dtype, dimensions, out);\n  const outReshaped =\n      reshape({inputs: {x: out}, backend, attrs: {shape: outShape}});\n  intermediates.push(out);\n  for (const i of intermediates) {\n    backend.disposeData(i.dataId);\n  }\n  return outReshaped;\n}\n"],"mappings":"AAAA;;;;;;;;;;;;;;;;AAiBA,SAAsBA,cAAc,EAAEC,GAAG,EAAcC,IAAI,QAAO,uBAAuB;AAGzF,SAAQC,mBAAmB,QAAO,yBAAyB;AAC3D,SAAQC,mBAAmB,QAAO,yBAAyB;AAC3D,SAAQC,4BAA4B,QAAO,oCAAoC;AAC/E,SAAQC,qBAAqB,EAAEC,mBAAmB,QAAO,yBAAyB;AAElF,SAAQC,iBAAiB,QAAO,gBAAgB;AAEhD,SAAQC,IAAI,QAAO,QAAQ;AAC3B,SAAQC,OAAO,QAAO,WAAW;AAcjC,OAAM,SAAUC,eAAeA,CAAC;EAC9BC,CAAC;EACDC,CAAC;EACDC,UAAU;EACVC,UAAU;EACVC,OAAO;EACPC,IAAI,GAAG,IAAI;EACXC,sBAAsB,GAAG,IAAI;EAC7BC,cAAc,GAAG,CAAC;EAClBC,UAAU,GAAG;AAAI,CACC;EAClB,MAAMC,KAAK,GAAGT,CAAC,CAACU,KAAK,CAACC,MAAM;EAC5B,MAAMC,KAAK,GAAGX,CAAC,CAACS,KAAK,CAACC,MAAM;EAE5B,MAAME,WAAW,GAAGX,UAAU,GAAGF,CAAC,CAACU,KAAK,CAACD,KAAK,GAAG,CAAC,CAAC,GAAGT,CAAC,CAACU,KAAK,CAACD,KAAK,GAAG,CAAC,CAAC;EACxE,MAAMK,WAAW,GAAGX,UAAU,GAAGF,CAAC,CAACS,KAAK,CAACE,KAAK,GAAG,CAAC,CAAC,GAAGX,CAAC,CAACS,KAAK,CAACE,KAAK,GAAG,CAAC,CAAC;EAExE,MAAMG,WAAW,GAAGb,UAAU,GAAGF,CAAC,CAACU,KAAK,CAACD,KAAK,GAAG,CAAC,CAAC,GAAGT,CAAC,CAACU,KAAK,CAACD,KAAK,GAAG,CAAC,CAAC;EACxE,MAAMO,WAAW,GAAGb,UAAU,GAAGF,CAAC,CAACS,KAAK,CAACE,KAAK,GAAG,CAAC,CAAC,GAAGX,CAAC,CAACS,KAAK,CAACE,KAAK,GAAG,CAAC,CAAC;EAExE,MAAMK,UAAU,GAAGjB,CAAC,CAACU,KAAK,CAACQ,KAAK,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC;EACvC,MAAMC,UAAU,GAAGlB,CAAC,CAACS,KAAK,CAACQ,KAAK,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC;EAEvC,MAAME,SAAS,GAAG9B,IAAI,CAAC+B,aAAa,CAACJ,UAAU,CAAC;EAChD,MAAMK,SAAS,GAAGhC,IAAI,CAAC+B,aAAa,CAACF,UAAU,CAAC;EAEhD,MAAMI,iBAAiB,GAAGnC,cAAc,CAACoC,0BAA0B,CAC/DxB,CAAC,CAACU,KAAK,CAACQ,KAAK,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,EAAEjB,CAAC,CAACS,KAAK,CAACQ,KAAK,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC;EAC/C,MAAMO,QAAQ,GAAGF,iBAAiB,CAACG,MAAM,CAAC,CAACX,WAAW,EAAEC,WAAW,CAAC,CAAC;EAErE1B,IAAI,CAACqC,MAAM,CACPd,WAAW,KAAKC,WAAW,EAC3B,MAAM,kCAAkCD,WAAW,SAAS,GACxD,GAAGC,WAAW,4BAA4Bd,CAAC,CAACU,KAAK,OAAO,GACxD,GAAGT,CAAC,CAACS,KAAK,mBAAmBR,UAAU,EAAE,GACzC,mBAAmBC,UAAU,cAAc,CAAC;EAEpD,MAAMyB,QAAQ,GAA6B1B,UAAU,GACjD,CAACkB,SAAS,EAAEP,WAAW,EAAEE,WAAW,CAAC,GACrC,CAACK,SAAS,EAAEL,WAAW,EAAEF,WAAW,CAAC;EACzC,MAAMgB,QAAQ,GAA6B1B,UAAU,GACjD,CAACmB,SAAS,EAAEN,WAAW,EAAEF,WAAW,CAAC,GACrC,CAACQ,SAAS,EAAER,WAAW,EAAEE,WAAW,CAAC;EAEzC;EACA,MAAMc,GAAG,GAAGhC,OAAO,CAAC;IAACiC,MAAM,EAAE;MAACC,CAAC,EAAEhC;IAAC,CAAC;IAAEI,OAAO;IAAE6B,KAAK,EAAE;MAACvB,KAAK,EAAEkB;IAAQ;EAAC,CAAC,CAAC;EACxE,MAAMM,GAAG,GAAGpC,OAAO,CAAC;IAACiC,MAAM,EAAE;MAACC,CAAC,EAAE/B;IAAC,CAAC;IAAEG,OAAO;IAAE6B,KAAK,EAAE;MAACvB,KAAK,EAAEmB;IAAQ;EAAC,CAAC,CAAC;EACxE,MAAMM,aAAa,GAAiB,CAACL,GAAG,EAAEI,GAAG,CAAC;EAE9C,MAAME,QAAQ,GAAGC,IAAI,CAACC,GAAG,CAAClB,SAAS,EAAEE,SAAS,CAAC;EAE/C,MAAMS,MAAM,GAAiB,CAACD,GAAG,EAAEI,GAAG,CAAC;EACvC,MAAMK,UAAU,GAAG,CACjB;IAACC,IAAI,EAAE,OAAO;IAAEC,IAAI,EAAE,CAAC1B,WAAW;EAAC,CAAC,EAAE;IAACyB,IAAI,EAAE,OAAO;IAAEC,IAAI,EAAE,CAACzB,WAAW;EAAC,CAAC,EAC1E;IAACwB,IAAI,EAAE,OAAO;IAAEC,IAAI,EAAE,CAAC5B,WAAW;EAAC,CAAC,CACrC;EAED,IAAI6B,OAAsB;EAC1B,IAAIC,GAAe;EACnB,MAAMC,WAAW,GACb,CAACR,QAAQ,EAAErB,WAAW,EAAEC,WAAW,CAAC;EACxC,IAAI6B,iBAAiB,GAAGxD,GAAG,EAAE,CAACyD,GAAG,CAAC,4BAA4B,CAAW;EACzE,IAAID,iBAAiB,GAAG,CAAC,EAAE;IACzB;IACA;IACA;IACA;IACA;IACA;IACA;IACA;IACA;IACA;IACA,MAAME,kBAAkB,GACpB1D,GAAG,EAAE,CAAC2D,SAAS,CAAC,oDAAoD,CAAC;IACzE,MAAMC,6BAA6B,GAAGF,kBAAkB,GAAG,CAAC,GACxDA,kBAAkB,GAClB3C,OAAO,CAAC6C,6BAA6B;IACzC,MAAMC,iBAAiB,GACnBd,QAAQ,GAAGC,IAAI,CAACc,IAAI,CAACpC,WAAW,GAAG,EAAE,CAAC,GAAGsB,IAAI,CAACc,IAAI,CAACnC,WAAW,GAAG,EAAE,CAAC;IACxE,MAAMoC,gBAAgB,GAClBF,iBAAiB,IAAID,6BAA6B,IACjDlC,WAAW,IAAI,CAAC,IAChBmC,iBAAiB,IAAID,6BAA6B,GAAG,CAAE;IAC5D,IAAIG,gBAAgB,EAAE;MACpB,IAAIhB,QAAQ,GAAGrB,WAAW,GAAGC,WAAW,IAAI,GAAG,EAAE;QAC/C6B,iBAAiB,GAAGjD,iBAAiB,CAACJ,mBAAmB;OAC1D,MAAM,IAAI4C,QAAQ,KAAK,CAAC,IAAItB,WAAW,IAAI,IAAI,EAAE;QAChD+B,iBAAiB,GAAGjD,iBAAiB,CAACD,mBAAmB;OAC1D,MAAM;QACLkD,iBAAiB,GAAGjD,iBAAiB,CAACH,4BAA4B;;KAErE,MAAM;MACLoD,iBAAiB,GAAGjD,iBAAiB,CAACL,mBAAmB;;;EAI7D,QAAQsD,iBAAiB;IACvB,KAAKjD,iBAAiB,CAACJ,mBAAmB;MACxCkD,OAAO,GAAG,IAAIlD,mBAAmB,CAC7BoD,WAAW,EAAE1C,UAAU,EAAEC,UAAU,EAAEE,IAAI,EAAEG,UAAU,EACrDF,sBAAsB,CAAC;MAC3B;IACF,KAAKV,iBAAiB,CAACD,mBAAmB;MAAE;QAC1C;QACA;QACAgD,GAAG,GAAG9C,IAAI,CACN;UAACO,OAAO;UAAE6B,KAAK,EAAE;YAACvB,KAAK,EAAEkC,WAAW;YAAES,KAAK,EAAE,CAAC;YAAEC,KAAK,EAAEtD,CAAC,CAACsD;UAAK;QAAC,CAAC,CAAC;QACrEZ,OAAO,GAAG,IAAI/C,mBAAmB,CAC7BiD,WAAW,EAAE9B,WAAW,EAAEZ,UAAU,EAAEC,UAAU,CAAC;QACrD,IAAIE,IAAI,IAAIG,UAAU,EAAE;UACtBmC,GAAG,GACCvC,OAAO,CAACmD,gBAAgB,CAACb,OAAO,EAAEX,MAAM,EAAE/B,CAAC,CAACsD,KAAK,EAAEf,UAAU,EAAEI,GAAG,CAAC;UACvE,MAAMa,qBAAqB,GAAG,IAAI9D,qBAAqB,CACnDiD,GAAG,CAACjC,KAAK,EAAEL,IAAI,EAAEG,UAAU,EAAEF,sBAAsB,CAAC;UACxD,IAAImD,WAAW,GAAG,IAAI;UACtB,MAAMC,gBAAgB,GAAiB,CAACf,GAAG,CAAC;UAC5C,IAAItC,IAAI,EAAE;YACRqD,gBAAgB,CAACC,IAAI,CAACtD,IAAI,CAAC;;UAE7B,IAAIC,sBAAsB,EAAE;YAC1BoD,gBAAgB,CAACC,IAAI,CAACrD,sBAAsB,CAAC;;UAE/C,IAAIE,UAAU,KAAK,WAAW,EAAE;YAC9BiD,WAAW,GAAG,CAAC;cAACjB,IAAI,EAAE,SAAS;cAAEC,IAAI,EAAE,CAAClC,cAAc;YAAC,CAAC,CAAC;YACzDiD,qBAAqB,CAACI,QAAQ,IAAI,eAAe;;UAEnD,MAAMC,YAAY,GAAGzD,OAAO,CAACmD,gBAAgB,CACzCC,qBAAqB,EAAEE,gBAAgB,EAAEf,GAAG,CAACW,KAAK,EAAEG,WAAW,CAAC;UACpEtB,aAAa,CAACwB,IAAI,CAAChB,GAAG,CAAC;UACvB,MAAMmB,WAAW,GAAGhE,OAAO,CACvB;YAACiC,MAAM,EAAE;cAACC,CAAC,EAAE6B;YAAY,CAAC;YAAEzD,OAAO;YAAE6B,KAAK,EAAE;cAACvB,KAAK,EAAEe;YAAQ;UAAC,CAAC,CAAC;UACnEU,aAAa,CAACwB,IAAI,CAACE,YAAY,CAAC;UAChC,KAAK,MAAME,CAAC,IAAI5B,aAAa,EAAE;YAC7B/B,OAAO,CAAC4D,WAAW,CAACD,CAAC,CAACE,MAAM,CAAC;;UAE/B,OAAOH,WAAW;;QAEpB;;IAEF,KAAKlE,iBAAiB,CAACH,4BAA4B;MACjDiD,OAAO,GAAG,IAAIjD,4BAA4B,CACtCmC,QAAQ,EAAEC,QAAQ,EAAEe,WAAW,EAAE1C,UAAU,EAAEC,UAAU,EAAEE,IAAI,EAC7DG,UAAU,EAAEF,sBAAsB,CAAC;MACvC;IACF,KAAKV,iBAAiB,CAACL,mBAAmB;MACxC;MACA;MACA,MAAM2E,yBAAyB,GAAG9D,OAAO,CAAC+D,WAAW,CAACC,OAAO,EAAE;MAC/D1B,OAAO,GAAG,IAAInD,mBAAmB,CAC7BqC,QAAQ,EAAEgB,WAAW,EAAE1C,UAAU,EAAEC,UAAU,EAAEE,IAAI,EAAEG,UAAU,EAC/DF,sBAAsB,EAAE4D,yBAAyB,CAAC;MACtD;IACF;MACE,MAAM,IAAIG,KAAK,CAAC,iCAAiCxB,iBAAiB,GAAG,CAAC;;EAG1E,IAAIxC,IAAI,EAAE;IACR0B,MAAM,CAAC4B,IAAI,CAACtD,IAAI,CAAC;;EAEnB,IAAIC,sBAAsB,EAAE;IAC1ByB,MAAM,CAAC4B,IAAI,CAACrD,sBAAsB,CAAC;;EAErC,IAAIE,UAAU,KAAK,WAAW,EAAE;IAC9B+B,UAAU,CAACoB,IAAI,CAAC;MAACnB,IAAI,EAAE,SAAS;MAAEC,IAAI,EAAE,CAAClC,cAAc;IAAC,CAAC,CAAC;IAC1DmC,OAAO,CAACkB,QAAQ,IAAI,eAAe;;EAErCjB,GAAG,GAAGvC,OAAO,CAACmD,gBAAgB,CAACb,OAAO,EAAEX,MAAM,EAAE/B,CAAC,CAACsD,KAAK,EAAEf,UAAU,EAAEI,GAAG,CAAC;EACzE,MAAMmB,WAAW,GACbhE,OAAO,CAAC;IAACiC,MAAM,EAAE;MAACC,CAAC,EAAEW;IAAG,CAAC;IAAEvC,OAAO;IAAE6B,KAAK,EAAE;MAACvB,KAAK,EAAEe;IAAQ;EAAC,CAAC,CAAC;EAClEU,aAAa,CAACwB,IAAI,CAAChB,GAAG,CAAC;EACvB,KAAK,MAAMoB,CAAC,IAAI5B,aAAa,EAAE;IAC7B/B,OAAO,CAAC4D,WAAW,CAACD,CAAC,CAACE,MAAM,CAAC;;EAE/B,OAAOH,WAAW;AACpB","ignoreList":[]},"metadata":{},"sourceType":"module","externalDependencies":[]}