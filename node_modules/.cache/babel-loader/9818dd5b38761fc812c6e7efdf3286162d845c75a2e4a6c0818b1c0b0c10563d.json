{"ast":null,"code":"/**\n * @license\n * Copyright 2019 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { util } from '@tensorflow/tfjs-core';\nconst arrayProduct = arr => {\n  let product = 1;\n  for (let i = 0; i < arr.length; i++) {\n    product *= arr[i];\n  }\n  return product;\n};\nexport function tilesFitEvenlyIntoShape(tileSize, shape) {\n  if (tileSize.length !== shape.length) {\n    throw new Error(`Cannot compute whether rank ${tileSize.length}` + ` tiles fit evenly into rank ${shape.length} shape` + ` - ranks must match.`);\n  }\n  return shape.every((dim, dimIdx) => dim % tileSize[dimIdx] === 0);\n}\n// Computes dispatch geometry based on layout of output dimensions and\n// workgroupSize.\nexport function computeDispatch(layout, outputShape, workgroupSize = [1, 1, 1], elementsPerThread = [1, 1, 1]) {\n  const [dispatchX, dispatchY, dispatchZ] = [Math.ceil(arrayProduct(layout.x.map(d => outputShape[d])) / (workgroupSize[0] * elementsPerThread[0])), layout.y ? Math.ceil(arrayProduct(layout.y.map(d => outputShape[d])) / (workgroupSize[1] * elementsPerThread[1])) : 1, layout.z ? Math.ceil(arrayProduct(layout.z.map(d => outputShape[d])) / (workgroupSize[2] * elementsPerThread[2])) : 1];\n  return [dispatchX, dispatchY, dispatchZ];\n}\nexport function computeWorkgroupInfoForMatMul(dimAOuter, dimInner, dimBOuter, transposeA = false) {\n  // These are experimental values. Usually, we need to adjust the work group\n  // size based on the input shapes to improve the EU occupancy.\n  // TODO: WebGPU limits the maximum allowed shared memory size as 16K. To make\n  // sure it doesn't exceed this limitations. Temporarily reduce the work group\n  // size to [8, 8, 1] and the work per thread size is [4, 4, 1]. But we should\n  // revisit it and find the balance between work group size and work per thread\n  // size.\n  const workgroupSize = [8, 8, 1];\n  const elementsPerThread = [4, 4, 1];\n  if (!transposeA) {\n    if (dimAOuter <= 8) {\n      elementsPerThread[1] = 1;\n    }\n    if (dimInner <= 16 && dimBOuter <= 16) {\n      workgroupSize[0] = 4;\n    }\n  }\n  return {\n    workgroupSize,\n    elementsPerThread\n  };\n}\nexport function computeWorkgroupSizeForConv2d(layout, outputShape, isVec4 = false) {\n  if (isVec4) {\n    return [8, 8, 1];\n  }\n  const dim0 = arrayProduct(layout.x.map(d => outputShape[d]));\n  const dim1 = arrayProduct(layout.y.map(d => outputShape[d]));\n  // TODO(jiajia.qin@intel.com): More fine tune based on outputShape.\n  // These are experimental values. Usually, we need to adjust the work group\n  // size based on the output shape. For example, when one dimension is smaller\n  // than 4, it will be wasteful if we assign a larger size for this dimension,\n  // which results lots of threads doing useless work and reduces parallelism\n  // of hardware threads. But it is always a balance between work group size\n  // and shared memory. If one dimension is too small, such as 1, shared memory\n  // will won't be fully utilized.\n  if (dim0 <= 4) {\n    return [4, 16, 1];\n  }\n  if (dim1 <= 4) {\n    return [16, 4, 1];\n  }\n  return [16, 16, 1];\n}\nexport function computeWorkPerThreadForConv2d(layout, outputShape, isVec4 = false) {\n  if (isVec4) {\n    return [4, 4, 1];\n  }\n  const dim0 = arrayProduct(layout.x.map(d => outputShape[d]));\n  const dim1 = arrayProduct(layout.y.map(d => outputShape[d]));\n  // TODO(jiajia.qin@intel.com): More fine tune based on outputShape.\n  // The following conditions correspond to the values set in\n  // computeWorkgroupSizeForConv2d.\n  if (dim0 <= 4) {\n    return [1, 2, 1];\n  }\n  if (dim1 <= 4) {\n    return [2, 1, 1];\n  }\n  return [2, 2, 1];\n}\nexport function flatDispatchLayout(shape) {\n  return {\n    x: shape.map((d, i) => i)\n  };\n}\nexport function GPUBytesPerElement(dtype) {\n  if (dtype === 'float32' || dtype === 'int32' || dtype === 'bool' || dtype === 'string') {\n    return 4;\n  } else if (dtype === 'complex64') {\n    return 8;\n  } else {\n    throw new Error(`Unknown dtype ${dtype}`);\n  }\n}\nexport function isWebGPUSupported() {\n  return !!(typeof globalThis !== 'undefined' && globalThis.navigator && globalThis.navigator.gpu);\n}\nexport function assertNotComplex(tensor, opName) {\n  if (!Array.isArray(tensor)) {\n    tensor = [tensor];\n  }\n  tensor.forEach(t => {\n    if (t != null) {\n      util.assert(t.dtype !== 'complex64', () => `${opName} does not support complex64 tensors ` + 'in the WebGPU backend.');\n    }\n  });\n}\nexport var MatMulProgramType;\n(function (MatMulProgramType) {\n  MatMulProgramType[MatMulProgramType[\"MatMulReduceProgram\"] = 0] = \"MatMulReduceProgram\";\n  MatMulProgramType[MatMulProgramType[\"MatMulSplitKProgram\"] = 1] = \"MatMulSplitKProgram\";\n  MatMulProgramType[MatMulProgramType[\"MatMulSmallOutputSizeProgram\"] = 2] = \"MatMulSmallOutputSizeProgram\";\n  MatMulProgramType[MatMulProgramType[\"MatMulPackedProgram\"] = 3] = \"MatMulPackedProgram\";\n  MatMulProgramType[MatMulProgramType[\"MatMulMax\"] = 4] = \"MatMulMax\";\n})(MatMulProgramType || (MatMulProgramType = {}));","map":{"version":3,"names":["util","arrayProduct","arr","product","i","length","tilesFitEvenlyIntoShape","tileSize","shape","Error","every","dim","dimIdx","computeDispatch","layout","outputShape","workgroupSize","elementsPerThread","dispatchX","dispatchY","dispatchZ","Math","ceil","x","map","d","y","z","computeWorkgroupInfoForMatMul","dimAOuter","dimInner","dimBOuter","transposeA","computeWorkgroupSizeForConv2d","isVec4","dim0","dim1","computeWorkPerThreadForConv2d","flatDispatchLayout","GPUBytesPerElement","dtype","isWebGPUSupported","globalThis","navigator","gpu","assertNotComplex","tensor","opName","Array","isArray","forEach","t","assert","MatMulProgramType"],"sources":["/Users/jesvinblazegmail.com/PycharmProjects/tfjs-backend-webgpu/src/webgpu_util.ts"],"sourcesContent":["/**\n * @license\n * Copyright 2019 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport {DataType, TensorInfo, util} from '@tensorflow/tfjs-core';\n\nconst arrayProduct = (arr: number[]) => {\n  let product = 1;\n  for (let i = 0; i < arr.length; i++) {\n    product *= arr[i];\n  }\n  return product;\n};\n\nexport function tilesFitEvenlyIntoShape(\n    tileSize: number[], shape: number[]): boolean {\n  if (tileSize.length !== shape.length) {\n    throw new Error(\n        `Cannot compute whether rank ${tileSize.length}` +\n        ` tiles fit evenly into rank ${shape.length} shape` +\n        ` - ranks must match.`);\n  }\n  return shape.every(\n      (dim: number, dimIdx: number) => dim % tileSize[dimIdx] === 0);\n}\n\n// Computes dispatch geometry based on layout of output dimensions and\n// workgroupSize.\nexport function computeDispatch(\n    layout: {x: number[], y?: number[], z?: number[]}, outputShape: number[],\n    workgroupSize: [number, number, number] = [1, 1, 1],\n    elementsPerThread: [number, number, number] =\n        [1, 1, 1]): [number, number, number] {\n  const [dispatchX, dispatchY, dispatchZ] = [\n    Math.ceil(\n        arrayProduct(layout.x.map(d => outputShape[d])) /\n        (workgroupSize[0] * elementsPerThread[0])),\n    layout.y ? Math.ceil(\n                   arrayProduct(layout.y.map(d => outputShape[d])) /\n                   (workgroupSize[1] * elementsPerThread[1])) :\n               1,\n    layout.z ? Math.ceil(\n                   arrayProduct(layout.z.map(d => outputShape[d])) /\n                   (workgroupSize[2] * elementsPerThread[2])) :\n               1\n  ];\n  return [dispatchX, dispatchY, dispatchZ];\n}\n\nexport type WorkgroupInfo = {\n  workgroupSize: [number, number, number],\n  elementsPerThread: [number, number, number],\n};\n\nexport function computeWorkgroupInfoForMatMul(\n    dimAOuter: number, dimInner: number, dimBOuter: number,\n    transposeA = false): WorkgroupInfo {\n  // These are experimental values. Usually, we need to adjust the work group\n  // size based on the input shapes to improve the EU occupancy.\n  // TODO: WebGPU limits the maximum allowed shared memory size as 16K. To make\n  // sure it doesn't exceed this limitations. Temporarily reduce the work group\n  // size to [8, 8, 1] and the work per thread size is [4, 4, 1]. But we should\n  // revisit it and find the balance between work group size and work per thread\n  // size.\n  const workgroupSize: [number, number, number] = [8, 8, 1];\n  const elementsPerThread: [number, number, number] = [4, 4, 1];\n\n  if (!transposeA) {\n    if (dimAOuter <= 8) {\n      elementsPerThread[1] = 1;\n    }\n\n    if (dimInner <= 16 && dimBOuter <= 16) {\n      workgroupSize[0] = 4;\n    }\n  }\n\n  return {workgroupSize, elementsPerThread};\n}\n\nexport function computeWorkgroupSizeForConv2d(\n    layout: {x: number[], y?: number[], z?: number[]}, outputShape: number[],\n    isVec4 = false): [number, number, number] {\n  if (isVec4) {\n    return [8, 8, 1];\n  }\n\n  const dim0 = arrayProduct(layout.x.map(d => outputShape[d]));\n  const dim1 = arrayProduct(layout.y.map(d => outputShape[d]));\n  // TODO(jiajia.qin@intel.com): More fine tune based on outputShape.\n  // These are experimental values. Usually, we need to adjust the work group\n  // size based on the output shape. For example, when one dimension is smaller\n  // than 4, it will be wasteful if we assign a larger size for this dimension,\n  // which results lots of threads doing useless work and reduces parallelism\n  // of hardware threads. But it is always a balance between work group size\n  // and shared memory. If one dimension is too small, such as 1, shared memory\n  // will won't be fully utilized.\n  if (dim0 <= 4) {\n    return [4, 16, 1];\n  }\n  if (dim1 <= 4) {\n    return [16, 4, 1];\n  }\n\n  return [16, 16, 1];\n}\n\nexport function computeWorkPerThreadForConv2d(\n    layout: {x: number[], y?: number[], z?: number[]}, outputShape: number[],\n    isVec4 = false): [number, number, number] {\n  if (isVec4) {\n    return [4, 4, 1];\n  }\n\n  const dim0 = arrayProduct(layout.x.map(d => outputShape[d]));\n  const dim1 = arrayProduct(layout.y.map(d => outputShape[d]));\n  // TODO(jiajia.qin@intel.com): More fine tune based on outputShape.\n  // The following conditions correspond to the values set in\n  // computeWorkgroupSizeForConv2d.\n  if (dim0 <= 4) {\n    return [1, 2, 1];\n  }\n  if (dim1 <= 4) {\n    return [2, 1, 1];\n  }\n\n  return [2, 2, 1];\n}\n\nexport function flatDispatchLayout(shape: number[]) {\n  return {x: shape.map((d, i) => i)};\n}\n\nexport function GPUBytesPerElement(dtype: DataType): number {\n  if (dtype === 'float32' || dtype === 'int32' || dtype === 'bool' ||\n      dtype === 'string') {\n    return 4;\n  } else if (dtype === 'complex64') {\n    return 8;\n  } else {\n    throw new Error(`Unknown dtype ${dtype}`);\n  }\n}\n\nexport function isWebGPUSupported(): boolean {\n  return !!(typeof globalThis !== 'undefined' && (globalThis.navigator)\n    && (globalThis.navigator.gpu));\n}\n\nexport function assertNotComplex(\n    tensor: TensorInfo|TensorInfo[], opName: string): void {\n  if (!Array.isArray(tensor)) {\n    tensor = [tensor];\n  }\n  tensor.forEach(t => {\n    if (t != null) {\n      util.assert(\n          t.dtype !== 'complex64',\n          () => `${opName} does not support complex64 tensors ` +\n              'in the WebGPU backend.');\n    }\n  });\n}\n\nexport enum MatMulProgramType {\n  MatMulReduceProgram,\n  MatMulSplitKProgram,\n  MatMulSmallOutputSizeProgram,\n  MatMulPackedProgram,\n  MatMulMax\n}\n"],"mappings":"AAAA;;;;;;;;;;;;;;;;AAgBA,SAA8BA,IAAI,QAAO,uBAAuB;AAEhE,MAAMC,YAAY,GAAIC,GAAa,IAAI;EACrC,IAAIC,OAAO,GAAG,CAAC;EACf,KAAK,IAAIC,CAAC,GAAG,CAAC,EAAEA,CAAC,GAAGF,GAAG,CAACG,MAAM,EAAED,CAAC,EAAE,EAAE;IACnCD,OAAO,IAAID,GAAG,CAACE,CAAC,CAAC;;EAEnB,OAAOD,OAAO;AAChB,CAAC;AAED,OAAM,SAAUG,uBAAuBA,CACnCC,QAAkB,EAAEC,KAAe;EACrC,IAAID,QAAQ,CAACF,MAAM,KAAKG,KAAK,CAACH,MAAM,EAAE;IACpC,MAAM,IAAII,KAAK,CACX,+BAA+BF,QAAQ,CAACF,MAAM,EAAE,GAChD,+BAA+BG,KAAK,CAACH,MAAM,QAAQ,GACnD,sBAAsB,CAAC;;EAE7B,OAAOG,KAAK,CAACE,KAAK,CACd,CAACC,GAAW,EAAEC,MAAc,KAAKD,GAAG,GAAGJ,QAAQ,CAACK,MAAM,CAAC,KAAK,CAAC,CAAC;AACpE;AAEA;AACA;AACA,OAAM,SAAUC,eAAeA,CAC3BC,MAAiD,EAAEC,WAAqB,EACxEC,aAAA,GAA0C,CAAC,CAAC,EAAE,CAAC,EAAE,CAAC,CAAC,EACnDC,iBAAA,GACI,CAAC,CAAC,EAAE,CAAC,EAAE,CAAC,CAAC;EACf,MAAM,CAACC,SAAS,EAAEC,SAAS,EAAEC,SAAS,CAAC,GAAG,CACxCC,IAAI,CAACC,IAAI,CACLrB,YAAY,CAACa,MAAM,CAACS,CAAC,CAACC,GAAG,CAACC,CAAC,IAAIV,WAAW,CAACU,CAAC,CAAC,CAAC,CAAC,IAC9CT,aAAa,CAAC,CAAC,CAAC,GAAGC,iBAAiB,CAAC,CAAC,CAAC,CAAC,CAAC,EAC9CH,MAAM,CAACY,CAAC,GAAGL,IAAI,CAACC,IAAI,CACLrB,YAAY,CAACa,MAAM,CAACY,CAAC,CAACF,GAAG,CAACC,CAAC,IAAIV,WAAW,CAACU,CAAC,CAAC,CAAC,CAAC,IAC9CT,aAAa,CAAC,CAAC,CAAC,GAAGC,iBAAiB,CAAC,CAAC,CAAC,CAAC,CAAC,GAC9C,CAAC,EACZH,MAAM,CAACa,CAAC,GAAGN,IAAI,CAACC,IAAI,CACLrB,YAAY,CAACa,MAAM,CAACa,CAAC,CAACH,GAAG,CAACC,CAAC,IAAIV,WAAW,CAACU,CAAC,CAAC,CAAC,CAAC,IAC9CT,aAAa,CAAC,CAAC,CAAC,GAAGC,iBAAiB,CAAC,CAAC,CAAC,CAAC,CAAC,GAC9C,CAAC,CACb;EACD,OAAO,CAACC,SAAS,EAAEC,SAAS,EAAEC,SAAS,CAAC;AAC1C;AAOA,OAAM,SAAUQ,6BAA6BA,CACzCC,SAAiB,EAAEC,QAAgB,EAAEC,SAAiB,EACtDC,UAAU,GAAG,KAAK;EACpB;EACA;EACA;EACA;EACA;EACA;EACA;EACA,MAAMhB,aAAa,GAA6B,CAAC,CAAC,EAAE,CAAC,EAAE,CAAC,CAAC;EACzD,MAAMC,iBAAiB,GAA6B,CAAC,CAAC,EAAE,CAAC,EAAE,CAAC,CAAC;EAE7D,IAAI,CAACe,UAAU,EAAE;IACf,IAAIH,SAAS,IAAI,CAAC,EAAE;MAClBZ,iBAAiB,CAAC,CAAC,CAAC,GAAG,CAAC;;IAG1B,IAAIa,QAAQ,IAAI,EAAE,IAAIC,SAAS,IAAI,EAAE,EAAE;MACrCf,aAAa,CAAC,CAAC,CAAC,GAAG,CAAC;;;EAIxB,OAAO;IAACA,aAAa;IAAEC;EAAiB,CAAC;AAC3C;AAEA,OAAM,SAAUgB,6BAA6BA,CACzCnB,MAAiD,EAAEC,WAAqB,EACxEmB,MAAM,GAAG,KAAK;EAChB,IAAIA,MAAM,EAAE;IACV,OAAO,CAAC,CAAC,EAAE,CAAC,EAAE,CAAC,CAAC;;EAGlB,MAAMC,IAAI,GAAGlC,YAAY,CAACa,MAAM,CAACS,CAAC,CAACC,GAAG,CAACC,CAAC,IAAIV,WAAW,CAACU,CAAC,CAAC,CAAC,CAAC;EAC5D,MAAMW,IAAI,GAAGnC,YAAY,CAACa,MAAM,CAACY,CAAC,CAACF,GAAG,CAACC,CAAC,IAAIV,WAAW,CAACU,CAAC,CAAC,CAAC,CAAC;EAC5D;EACA;EACA;EACA;EACA;EACA;EACA;EACA;EACA,IAAIU,IAAI,IAAI,CAAC,EAAE;IACb,OAAO,CAAC,CAAC,EAAE,EAAE,EAAE,CAAC,CAAC;;EAEnB,IAAIC,IAAI,IAAI,CAAC,EAAE;IACb,OAAO,CAAC,EAAE,EAAE,CAAC,EAAE,CAAC,CAAC;;EAGnB,OAAO,CAAC,EAAE,EAAE,EAAE,EAAE,CAAC,CAAC;AACpB;AAEA,OAAM,SAAUC,6BAA6BA,CACzCvB,MAAiD,EAAEC,WAAqB,EACxEmB,MAAM,GAAG,KAAK;EAChB,IAAIA,MAAM,EAAE;IACV,OAAO,CAAC,CAAC,EAAE,CAAC,EAAE,CAAC,CAAC;;EAGlB,MAAMC,IAAI,GAAGlC,YAAY,CAACa,MAAM,CAACS,CAAC,CAACC,GAAG,CAACC,CAAC,IAAIV,WAAW,CAACU,CAAC,CAAC,CAAC,CAAC;EAC5D,MAAMW,IAAI,GAAGnC,YAAY,CAACa,MAAM,CAACY,CAAC,CAACF,GAAG,CAACC,CAAC,IAAIV,WAAW,CAACU,CAAC,CAAC,CAAC,CAAC;EAC5D;EACA;EACA;EACA,IAAIU,IAAI,IAAI,CAAC,EAAE;IACb,OAAO,CAAC,CAAC,EAAE,CAAC,EAAE,CAAC,CAAC;;EAElB,IAAIC,IAAI,IAAI,CAAC,EAAE;IACb,OAAO,CAAC,CAAC,EAAE,CAAC,EAAE,CAAC,CAAC;;EAGlB,OAAO,CAAC,CAAC,EAAE,CAAC,EAAE,CAAC,CAAC;AAClB;AAEA,OAAM,SAAUE,kBAAkBA,CAAC9B,KAAe;EAChD,OAAO;IAACe,CAAC,EAAEf,KAAK,CAACgB,GAAG,CAAC,CAACC,CAAC,EAAErB,CAAC,KAAKA,CAAC;EAAC,CAAC;AACpC;AAEA,OAAM,SAAUmC,kBAAkBA,CAACC,KAAe;EAChD,IAAIA,KAAK,KAAK,SAAS,IAAIA,KAAK,KAAK,OAAO,IAAIA,KAAK,KAAK,MAAM,IAC5DA,KAAK,KAAK,QAAQ,EAAE;IACtB,OAAO,CAAC;GACT,MAAM,IAAIA,KAAK,KAAK,WAAW,EAAE;IAChC,OAAO,CAAC;GACT,MAAM;IACL,MAAM,IAAI/B,KAAK,CAAC,iBAAiB+B,KAAK,EAAE,CAAC;;AAE7C;AAEA,OAAM,SAAUC,iBAAiBA,CAAA;EAC/B,OAAO,CAAC,EAAE,OAAOC,UAAU,KAAK,WAAW,IAAKA,UAAU,CAACC,SAAU,IAC/DD,UAAU,CAACC,SAAS,CAACC,GAAI,CAAC;AAClC;AAEA,OAAM,SAAUC,gBAAgBA,CAC5BC,MAA+B,EAAEC,MAAc;EACjD,IAAI,CAACC,KAAK,CAACC,OAAO,CAACH,MAAM,CAAC,EAAE;IAC1BA,MAAM,GAAG,CAACA,MAAM,CAAC;;EAEnBA,MAAM,CAACI,OAAO,CAACC,CAAC,IAAG;IACjB,IAAIA,CAAC,IAAI,IAAI,EAAE;MACbnD,IAAI,CAACoD,MAAM,CACPD,CAAC,CAACX,KAAK,KAAK,WAAW,EACvB,MAAM,GAAGO,MAAM,sCAAsC,GACjD,wBAAwB,CAAC;;EAErC,CAAC,CAAC;AACJ;AAEA,WAAYM,iBAMX;AAND,WAAYA,iBAAiB;EAC3BA,iBAAA,CAAAA,iBAAA,oDAAmB;EACnBA,iBAAA,CAAAA,iBAAA,oDAAmB;EACnBA,iBAAA,CAAAA,iBAAA,sEAA4B;EAC5BA,iBAAA,CAAAA,iBAAA,oDAAmB;EACnBA,iBAAA,CAAAA,iBAAA,gCAAS;AACX,CAAC,EANWA,iBAAiB,KAAjBA,iBAAiB","ignoreList":[]},"metadata":{},"sourceType":"module","externalDependencies":[]}