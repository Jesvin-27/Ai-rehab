{"ast":null,"code":"/**\n * @license\n * Copyright 2021 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { env } from '@tensorflow/tfjs-core';\nimport { Conv2DMMProgram } from '../conv2d_mm_webgpu';\nimport { Conv2DNaiveProgram } from '../conv2d_naive_webgpu';\nimport { Im2ColProgram } from '../im2col_webgpu';\nimport { batchMatMulImpl } from './BatchMatMul_impl';\nimport { reshape } from './Reshape';\n// conv2dByMatMul fuses height and width into one dimension to compute\n// batchMatMul, so bias and activation weights are also supposed to fuse the two\n// dimensions into one.\n//\n// This function computes the target shape for fusing height and width\n// dimensions. Returning null means the shape is already compatible.\nfunction getShapeForBatchMatMul(shape, isChannelsLast) {\n  const length = shape.length;\n  if (length >= 3) {\n    return isChannelsLast ? [...shape.slice(0, -3) /* batch */, shape[length - 3] * shape[length - 2] /* height * width */, shape[length - 1] /* channel */] : [...shape.slice(0, -3) /* batch */, shape[length - 3] /* channel */, shape[length - 2] * shape[length - 1] /* height * width */];\n  } else if (!isChannelsLast && length === 1 && shape[0] > 1) {\n    return [shape[0], 1];\n  } else {\n    return null;\n  }\n}\n// For 1x1 kernels that iterate through every point in the input, convolution\n// can be expressed as matrix multiplication (without need for memory\n// remapping).\nfunction conv2dByMatMul({\n  x,\n  filter,\n  convInfo,\n  backend,\n  bias = null,\n  preluActivationWeights = null,\n  leakyreluAlpha = 0,\n  activation = null\n}) {\n  const isChannelsLast = convInfo.dataFormat === 'channelsLast';\n  const transposeA = isChannelsLast ? false : true;\n  const transposeB = false;\n  const sameSize = isChannelsLast && convInfo.filterHeight === convInfo.inHeight && convInfo.filterWidth === convInfo.inWidth && convInfo.padInfo.type === 'VALID';\n  const intermediates = [];\n  let xReshaped;\n  let filterReshaped;\n  if (sameSize) {\n    const sharedDim = convInfo.inHeight * convInfo.inWidth * convInfo.inChannels;\n    xReshaped = reshape({\n      inputs: {\n        x\n      },\n      backend,\n      attrs: {\n        shape: [1, convInfo.batchSize, sharedDim]\n      }\n    });\n    filterReshaped = reshape({\n      inputs: {\n        x: filter\n      },\n      backend,\n      attrs: {\n        shape: [1, sharedDim, convInfo.outChannels]\n      }\n    });\n  } else {\n    xReshaped = reshape({\n      inputs: {\n        x\n      },\n      backend,\n      attrs: {\n        shape: isChannelsLast ? [convInfo.batchSize, convInfo.inHeight * convInfo.inWidth, convInfo.inChannels] : [convInfo.batchSize, convInfo.inChannels, convInfo.inHeight * convInfo.inWidth]\n      }\n    });\n    filterReshaped = reshape({\n      inputs: {\n        x: filter\n      },\n      backend,\n      attrs: {\n        shape: [1, convInfo.inChannels, convInfo.outChannels]\n      }\n    });\n  }\n  intermediates.push(xReshaped);\n  intermediates.push(filterReshaped);\n  if (preluActivationWeights != null) {\n    const targetShape = getShapeForBatchMatMul(preluActivationWeights.shape, isChannelsLast);\n    if (targetShape != null) {\n      preluActivationWeights = reshape({\n        inputs: {\n          x: preluActivationWeights\n        },\n        backend,\n        attrs: {\n          shape: targetShape\n        }\n      });\n      intermediates.push(preluActivationWeights);\n    }\n  }\n  if (bias != null) {\n    const targetShape = getShapeForBatchMatMul(bias.shape, isChannelsLast);\n    if (targetShape != null) {\n      bias = reshape({\n        inputs: {\n          x: bias\n        },\n        backend,\n        attrs: {\n          shape: targetShape\n        }\n      });\n      intermediates.push(bias);\n    }\n  }\n  const result = batchMatMulImpl({\n    a: isChannelsLast ? xReshaped : filterReshaped,\n    b: isChannelsLast ? filterReshaped : xReshaped,\n    transposeA,\n    transposeB,\n    backend,\n    bias,\n    activation,\n    preluActivationWeights,\n    leakyreluAlpha\n  });\n  const out = reshape({\n    inputs: {\n      x: result\n    },\n    backend,\n    attrs: {\n      shape: convInfo.outShape\n    }\n  });\n  intermediates.push(result);\n  for (const i of intermediates) {\n    backend.disposeData(i.dataId);\n  }\n  return out;\n}\n// Implements the im2col algorithm as outlined in \"High Performance\n// Convolutional Neural Networks for Document Processing\" (Suvisoft, 2006)\nfunction conv2dWithIm2Col({\n  x,\n  filter,\n  convInfo,\n  backend,\n  bias = null,\n  preluActivationWeights = null,\n  leakyreluAlpha = 0,\n  activation = null\n}) {\n  // Rearranges conv2d input so each block to be convolved over forms the\n  // row of a new matrix with shape [outHeight * outWidth,\n  // filterWidth * filterHeight * inChannels]. The filter is also rearranged so\n  // each output channel forms a col of a new matrix with shape [\n  // filterWidth * filterHeight * inChannels, outChannels]. The convolution is\n  // then computed by multiplying these matrices and reshaping the result.\n  const {\n    filterWidth,\n    filterHeight,\n    inChannels,\n    strideWidth,\n    strideHeight,\n    padInfo,\n    outWidth,\n    outHeight,\n    dilationWidth,\n    dilationHeight,\n    dataFormat\n  } = convInfo;\n  const isChannelsLast = dataFormat === 'channelsLast';\n  const sharedDim = filterWidth * filterHeight * inChannels;\n  const numCols = outHeight * outWidth;\n  const x2ColShape = isChannelsLast ? [convInfo.batchSize, numCols, sharedDim] : [convInfo.batchSize, sharedDim, numCols];\n  const im2ColProgram = new Im2ColProgram(x2ColShape, isChannelsLast);\n  const dimensions = [{\n    type: 'int32',\n    data: [padInfo.top, padInfo.left]\n  }, {\n    type: 'int32',\n    data: [strideHeight, strideWidth]\n  }, {\n    type: 'int32',\n    data: [dilationHeight, dilationWidth]\n  }, {\n    type: 'int32',\n    data: [outWidth]\n  }, {\n    type: 'int32',\n    data: [inChannels * filterWidth]\n  }, {\n    type: 'int32',\n    data: [inChannels]\n  }];\n  const x2Col = backend.runWebGPUProgram(im2ColProgram, [x], x.dtype, dimensions);\n  const intermediates = [];\n  intermediates.push(x2Col);\n  const filterReshaped = reshape({\n    inputs: {\n      x: filter\n    },\n    backend,\n    attrs: {\n      shape: [1, sharedDim, -1]\n    }\n  });\n  intermediates.push(filterReshaped);\n  if (preluActivationWeights != null) {\n    const targetShape = getShapeForBatchMatMul(preluActivationWeights.shape, isChannelsLast);\n    if (targetShape != null) {\n      preluActivationWeights = reshape({\n        inputs: {\n          x: preluActivationWeights\n        },\n        backend,\n        attrs: {\n          shape: targetShape\n        }\n      });\n      intermediates.push(preluActivationWeights);\n    }\n  }\n  if (bias != null) {\n    const targetShape = getShapeForBatchMatMul(bias.shape, isChannelsLast);\n    if (targetShape != null) {\n      bias = reshape({\n        inputs: {\n          x: bias\n        },\n        backend,\n        attrs: {\n          shape: targetShape\n        }\n      });\n      intermediates.push(bias);\n    }\n  }\n  const transposeA = isChannelsLast ? false : true;\n  const transposeB = false;\n  const result = batchMatMulImpl({\n    a: isChannelsLast ? x2Col : filterReshaped,\n    b: isChannelsLast ? filterReshaped : x2Col,\n    transposeA,\n    transposeB,\n    backend,\n    bias,\n    activation,\n    preluActivationWeights,\n    leakyreluAlpha\n  });\n  const out = reshape({\n    inputs: {\n      x: result\n    },\n    backend,\n    attrs: {\n      shape: convInfo.outShape\n    }\n  });\n  intermediates.push(result);\n  for (const i of intermediates) {\n    backend.disposeData(i.dataId);\n  }\n  return out;\n}\nexport function conv2DImpl({\n  x,\n  filter,\n  convInfo,\n  backend,\n  bias = null,\n  preluActivationWeights = null,\n  leakyreluAlpha = 0,\n  activation = null\n}) {\n  const hasBias = bias != null;\n  const hasPreluActivationWeights = preluActivationWeights != null;\n  const isChannelsLast = convInfo.dataFormat === 'channelsLast';\n  const sameSize = isChannelsLast && convInfo.filterHeight === convInfo.inHeight && convInfo.filterWidth === convInfo.inWidth && convInfo.padInfo.type === 'VALID';\n  const useNaiveConv2d = env().getBool('WEBGPU_USE_NAIVE_CONV2D_DEBUG');\n  if (!useNaiveConv2d && (sameSize || convInfo.filterHeight === 1 && convInfo.filterWidth === 1 && convInfo.dilationHeight === 1 && convInfo.dilationWidth === 1 && convInfo.strideHeight === 1 && convInfo.strideWidth === 1 && (convInfo.padInfo.type === 'SAME' || convInfo.padInfo.type === 'VALID'))) {\n    return conv2dByMatMul({\n      x,\n      filter,\n      convInfo,\n      backend,\n      bias,\n      activation,\n      preluActivationWeights,\n      leakyreluAlpha\n    });\n  }\n  const thresholdFlagValue = env().getNumber('WEBGPU_THRESHOLD_TO_INCREASE_WORKGROUPS_FOR_MATMUL');\n  const thresholdToIncreaseWorkgroups = thresholdFlagValue > -1 ? thresholdFlagValue : backend.thresholdToIncreaseWorkgroups;\n  const workgroupsBy32x32 = convInfo.batchSize * Math.ceil(convInfo.outHeight * convInfo.outWidth / 32) * Math.ceil(convInfo.outChannels / 32);\n  if (env().getBool('WEBGPU_CONV_SEPARATE_IM2COL_SHADER') || workgroupsBy32x32 <= thresholdToIncreaseWorkgroups) {\n    return conv2dWithIm2Col({\n      x,\n      filter,\n      convInfo,\n      backend,\n      bias,\n      preluActivationWeights,\n      leakyreluAlpha,\n      activation\n    });\n  }\n  let program;\n  const padInfo = [convInfo.padInfo.top, convInfo.padInfo.left];\n  const dimensions = [{\n    type: 'int32',\n    data: [convInfo.filterHeight, convInfo.filterWidth]\n  }, {\n    type: 'int32',\n    data: [...padInfo]\n  }, {\n    type: 'int32',\n    data: [convInfo.strideHeight, convInfo.strideWidth]\n  }, {\n    type: 'int32',\n    data: [convInfo.dilationHeight, convInfo.dilationWidth]\n  }];\n  if (useNaiveConv2d) {\n    program = new Conv2DNaiveProgram(convInfo, hasBias, activation, hasPreluActivationWeights);\n  } else {\n    const dimAOuter = isChannelsLast ? convInfo.outHeight * convInfo.outWidth : convInfo.outChannels;\n    const dimBOuter = isChannelsLast ? convInfo.outChannels : convInfo.outHeight * convInfo.outWidth;\n    const dimInner = convInfo.filterHeight * convInfo.filterWidth * convInfo.inChannels;\n    dimensions.push({\n      type: 'int32',\n      data: [dimAOuter]\n    }, {\n      type: 'int32',\n      data: [dimBOuter]\n    }, {\n      type: 'int32',\n      data: [dimInner]\n    });\n    // Experiments show that sequential access is more friendly for Intel GPUs.\n    const sequentialAccessByThreads = backend.adapterInfo.isIntel();\n    program = new Conv2DMMProgram(convInfo, dimAOuter, dimBOuter, dimInner, hasBias, activation, hasPreluActivationWeights, sequentialAccessByThreads);\n  }\n  const intermediates = [];\n  const inputVar = [x, filter];\n  if (hasBias) {\n    if (!isChannelsLast && bias.shape.length === 1) {\n      bias = reshape({\n        inputs: {\n          x: bias\n        },\n        backend,\n        attrs: {\n          shape: [bias.shape[0], 1, 1]\n        }\n      });\n      intermediates.push(bias);\n    }\n    inputVar.push(bias);\n  }\n  if (hasPreluActivationWeights) {\n    if (!isChannelsLast && preluActivationWeights.shape.length === 1) {\n      preluActivationWeights = reshape({\n        inputs: {\n          x: preluActivationWeights\n        },\n        backend,\n        attrs: {\n          shape: [preluActivationWeights.shape[0], 1, 1]\n        }\n      });\n      intermediates.push(preluActivationWeights);\n    }\n    inputVar.push(preluActivationWeights);\n  }\n  if (activation === 'leakyrelu') {\n    dimensions.push({\n      type: 'float32',\n      data: [leakyreluAlpha]\n    });\n    program.uniforms += ' alpha : f32,';\n  }\n  const out = backend.runWebGPUProgram(program, inputVar, x.dtype, dimensions);\n  for (const i of intermediates) {\n    backend.disposeData(i.dataId);\n  }\n  return out;\n}","map":{"version":3,"names":["env","Conv2DMMProgram","Conv2DNaiveProgram","Im2ColProgram","batchMatMulImpl","reshape","getShapeForBatchMatMul","shape","isChannelsLast","length","slice","conv2dByMatMul","x","filter","convInfo","backend","bias","preluActivationWeights","leakyreluAlpha","activation","dataFormat","transposeA","transposeB","sameSize","filterHeight","inHeight","filterWidth","inWidth","padInfo","type","intermediates","xReshaped","filterReshaped","sharedDim","inChannels","inputs","attrs","batchSize","outChannels","push","targetShape","result","a","b","out","outShape","i","disposeData","dataId","conv2dWithIm2Col","strideWidth","strideHeight","outWidth","outHeight","dilationWidth","dilationHeight","numCols","x2ColShape","im2ColProgram","dimensions","data","top","left","x2Col","runWebGPUProgram","dtype","conv2DImpl","hasBias","hasPreluActivationWeights","useNaiveConv2d","getBool","thresholdFlagValue","getNumber","thresholdToIncreaseWorkgroups","workgroupsBy32x32","Math","ceil","program","dimAOuter","dimBOuter","dimInner","sequentialAccessByThreads","adapterInfo","isIntel","inputVar","uniforms"],"sources":["/Users/jesvinblazegmail.com/PycharmProjects/tfjs-backend-webgpu/src/kernels/Conv2D_impl.ts"],"sourcesContent":["/**\n * @license\n * Copyright 2021 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {backend_util, env, TensorInfo} from '@tensorflow/tfjs-core';\n\nimport {WebGPUBackend} from '../backend_webgpu';\nimport {Conv2DMMProgram} from '../conv2d_mm_webgpu';\nimport {Conv2DNaiveProgram} from '../conv2d_naive_webgpu';\nimport {Im2ColProgram} from '../im2col_webgpu';\nimport {WebGPUProgram} from '../webgpu_program';\n\nimport {batchMatMulImpl} from './BatchMatMul_impl';\nimport {reshape} from './Reshape';\n\ntype Conv2DConfig = {\n  x: TensorInfo,\n  filter: TensorInfo,\n  convInfo: backend_util.Conv2DInfo,\n  backend: WebGPUBackend,\n  bias?: TensorInfo,\n  preluActivationWeights?: TensorInfo,\n  leakyreluAlpha?: number,\n  activation?: backend_util.Activation\n};\n\n// conv2dByMatMul fuses height and width into one dimension to compute\n// batchMatMul, so bias and activation weights are also supposed to fuse the two\n// dimensions into one.\n//\n// This function computes the target shape for fusing height and width\n// dimensions. Returning null means the shape is already compatible.\nfunction getShapeForBatchMatMul(\n    shape: number[], isChannelsLast: boolean): number[] {\n  const length = shape.length;\n  if (length >= 3) {\n    return isChannelsLast ?\n        [\n          ...shape.slice(0, -3) /* batch */,\n          shape[length - 3] * shape[length - 2] /* height * width */,\n          shape[length - 1] /* channel */\n        ] :\n        [\n          ...shape.slice(0, -3) /* batch */, shape[length - 3] /* channel */,\n          shape[length - 2] * shape[length - 1] /* height * width */\n        ];\n  } else if (!isChannelsLast && length === 1 && shape[0] > 1) {\n    return [shape[0], 1];\n  } else {\n    return null;\n  }\n}\n\n// For 1x1 kernels that iterate through every point in the input, convolution\n// can be expressed as matrix multiplication (without need for memory\n// remapping).\nfunction conv2dByMatMul({\n  x,\n  filter,\n  convInfo,\n  backend,\n  bias = null,\n  preluActivationWeights = null,\n  leakyreluAlpha = 0,\n  activation = null\n}: Conv2DConfig) {\n  const isChannelsLast = convInfo.dataFormat === 'channelsLast';\n  const transposeA = isChannelsLast ? false : true;\n  const transposeB = false;\n\n  const sameSize = isChannelsLast &&\n      convInfo.filterHeight === convInfo.inHeight &&\n      convInfo.filterWidth === convInfo.inWidth &&\n      convInfo.padInfo.type === 'VALID';\n  const intermediates: TensorInfo[] = [];\n  let xReshaped;\n  let filterReshaped;\n\n  if (sameSize) {\n    const sharedDim =\n        convInfo.inHeight * convInfo.inWidth * convInfo.inChannels;\n    xReshaped = reshape({\n      inputs: {x},\n      backend,\n      attrs: {shape: [1, convInfo.batchSize, sharedDim]}\n    });\n    filterReshaped = reshape({\n      inputs: {x: filter},\n      backend,\n      attrs: {shape: [1, sharedDim, convInfo.outChannels]}\n    });\n  } else {\n    xReshaped = reshape({\n      inputs: {x},\n      backend,\n      attrs: {\n        shape: isChannelsLast ?\n            [\n              convInfo.batchSize, convInfo.inHeight * convInfo.inWidth,\n              convInfo.inChannels\n            ] :\n            [\n              convInfo.batchSize, convInfo.inChannels,\n              convInfo.inHeight * convInfo.inWidth\n            ]\n      }\n    });\n    filterReshaped = reshape({\n      inputs: {x: filter},\n      backend,\n      attrs: {shape: [1, convInfo.inChannels, convInfo.outChannels]}\n    });\n  }\n  intermediates.push(xReshaped);\n  intermediates.push(filterReshaped);\n\n  if (preluActivationWeights != null) {\n    const targetShape =\n        getShapeForBatchMatMul(preluActivationWeights.shape, isChannelsLast);\n    if (targetShape != null) {\n      preluActivationWeights = reshape({\n        inputs: {x: preluActivationWeights},\n        backend,\n        attrs: {shape: targetShape}\n      });\n      intermediates.push(preluActivationWeights);\n    }\n  }\n\n  if (bias != null) {\n    const targetShape = getShapeForBatchMatMul(bias.shape, isChannelsLast);\n    if (targetShape != null) {\n      bias = reshape({inputs: {x: bias}, backend, attrs: {shape: targetShape}});\n      intermediates.push(bias);\n    }\n  }\n\n  const result = batchMatMulImpl({\n    a: isChannelsLast ? xReshaped : filterReshaped,\n    b: isChannelsLast ? filterReshaped : xReshaped,\n    transposeA,\n    transposeB,\n    backend,\n    bias,\n    activation,\n    preluActivationWeights,\n    leakyreluAlpha\n  });\n  const out = reshape(\n      {inputs: {x: result}, backend, attrs: {shape: convInfo.outShape}});\n  intermediates.push(result);\n\n  for (const i of intermediates) {\n    backend.disposeData(i.dataId);\n  }\n\n  return out;\n}\n\n// Implements the im2col algorithm as outlined in \"High Performance\n// Convolutional Neural Networks for Document Processing\" (Suvisoft, 2006)\nfunction conv2dWithIm2Col({\n  x,\n  filter,\n  convInfo,\n  backend,\n  bias = null,\n  preluActivationWeights = null,\n  leakyreluAlpha = 0,\n  activation = null\n}: Conv2DConfig) {\n  // Rearranges conv2d input so each block to be convolved over forms the\n  // row of a new matrix with shape [outHeight * outWidth,\n  // filterWidth * filterHeight * inChannels]. The filter is also rearranged so\n  // each output channel forms a col of a new matrix with shape [\n  // filterWidth * filterHeight * inChannels, outChannels]. The convolution is\n  // then computed by multiplying these matrices and reshaping the result.\n  const {\n    filterWidth,\n    filterHeight,\n    inChannels,\n    strideWidth,\n    strideHeight,\n    padInfo,\n    outWidth,\n    outHeight,\n    dilationWidth,\n    dilationHeight,\n    dataFormat\n  } = convInfo;\n\n  const isChannelsLast = dataFormat === 'channelsLast';\n\n  const sharedDim = filterWidth * filterHeight * inChannels;\n  const numCols = outHeight * outWidth;\n  const x2ColShape = isChannelsLast ? [convInfo.batchSize, numCols, sharedDim] :\n                                      [convInfo.batchSize, sharedDim, numCols];\n\n  const im2ColProgram = new Im2ColProgram(x2ColShape, isChannelsLast);\n  const dimensions = [\n    {type: 'int32', data: [padInfo.top, padInfo.left]},      // Padding.\n    {type: 'int32', data: [strideHeight, strideWidth]},      // Stride.\n    {type: 'int32', data: [dilationHeight, dilationWidth]},  // Dilation.\n    {type: 'int32', data: [outWidth]},\n    {type: 'int32', data: [inChannels * filterWidth]},  // itemsPerBlockRow.\n    {type: 'int32', data: [inChannels]}\n  ];\n  const x2Col =\n      backend.runWebGPUProgram(im2ColProgram, [x], x.dtype, dimensions);\n\n  const intermediates: TensorInfo[] = [];\n  intermediates.push(x2Col);\n\n  const filterReshaped = reshape(\n      {inputs: {x: filter}, backend, attrs: {shape: [1, sharedDim, -1]}});\n  intermediates.push(filterReshaped);\n\n  if (preluActivationWeights != null) {\n    const targetShape =\n        getShapeForBatchMatMul(preluActivationWeights.shape, isChannelsLast);\n    if (targetShape != null) {\n      preluActivationWeights = reshape({\n        inputs: {x: preluActivationWeights},\n        backend,\n        attrs: {shape: targetShape}\n      });\n      intermediates.push(preluActivationWeights);\n    }\n  }\n\n  if (bias != null) {\n    const targetShape = getShapeForBatchMatMul(bias.shape, isChannelsLast);\n    if (targetShape != null) {\n      bias = reshape({inputs: {x: bias}, backend, attrs: {shape: targetShape}});\n      intermediates.push(bias);\n    }\n  }\n\n  const transposeA = isChannelsLast ? false : true;\n  const transposeB = false;\n  const result = batchMatMulImpl({\n    a: isChannelsLast ? x2Col : filterReshaped,\n    b: isChannelsLast ? filterReshaped : x2Col,\n    transposeA,\n    transposeB,\n    backend,\n    bias,\n    activation,\n    preluActivationWeights,\n    leakyreluAlpha\n  });\n  const out = reshape(\n      {inputs: {x: result}, backend, attrs: {shape: convInfo.outShape}});\n  intermediates.push(result);\n  for (const i of intermediates) {\n    backend.disposeData(i.dataId);\n  }\n\n  return out;\n}\n\nexport function conv2DImpl({\n  x,\n  filter,\n  convInfo,\n  backend,\n  bias = null,\n  preluActivationWeights = null,\n  leakyreluAlpha = 0,\n  activation = null\n}: Conv2DConfig) {\n  const hasBias = bias != null;\n  const hasPreluActivationWeights = preluActivationWeights != null;\n  const isChannelsLast = convInfo.dataFormat === 'channelsLast';\n  const sameSize = isChannelsLast &&\n      convInfo.filterHeight === convInfo.inHeight &&\n      convInfo.filterWidth === convInfo.inWidth &&\n      convInfo.padInfo.type === 'VALID';\n  const useNaiveConv2d = env().getBool('WEBGPU_USE_NAIVE_CONV2D_DEBUG');\n\n  if (!useNaiveConv2d &&\n      (sameSize ||\n       (convInfo.filterHeight === 1 && convInfo.filterWidth === 1 &&\n        convInfo.dilationHeight === 1 && convInfo.dilationWidth === 1 &&\n        convInfo.strideHeight === 1 && convInfo.strideWidth === 1 &&\n        (convInfo.padInfo.type === 'SAME' ||\n         convInfo.padInfo.type === 'VALID')))) {\n    return conv2dByMatMul({\n      x,\n      filter,\n      convInfo,\n      backend,\n      bias,\n      activation,\n      preluActivationWeights,\n      leakyreluAlpha\n    });\n  }\n\n  const thresholdFlagValue =\n      env().getNumber('WEBGPU_THRESHOLD_TO_INCREASE_WORKGROUPS_FOR_MATMUL');\n  const thresholdToIncreaseWorkgroups = thresholdFlagValue > -1 ?\n      thresholdFlagValue :\n      backend.thresholdToIncreaseWorkgroups;\n  const workgroupsBy32x32 = convInfo.batchSize *\n      Math.ceil((convInfo.outHeight * convInfo.outWidth) / 32) *\n      Math.ceil(convInfo.outChannels / 32);\n  if (env().getBool('WEBGPU_CONV_SEPARATE_IM2COL_SHADER') ||\n      workgroupsBy32x32 <= thresholdToIncreaseWorkgroups) {\n    return conv2dWithIm2Col({\n      x,\n      filter,\n      convInfo,\n      backend,\n      bias,\n      preluActivationWeights,\n      leakyreluAlpha,\n      activation\n    });\n  }\n\n  let program: WebGPUProgram;\n  const padInfo = [convInfo.padInfo.top, convInfo.padInfo.left];\n  const dimensions = [\n    {type: 'int32', data: [convInfo.filterHeight, convInfo.filterWidth]},\n    {type: 'int32', data: [...padInfo]},\n    {type: 'int32', data: [convInfo.strideHeight, convInfo.strideWidth]},\n    {type: 'int32', data: [convInfo.dilationHeight, convInfo.dilationWidth]}\n  ];\n  if (useNaiveConv2d) {\n    program = new Conv2DNaiveProgram(\n        convInfo, hasBias, activation, hasPreluActivationWeights);\n  } else {\n    const dimAOuter = isChannelsLast ? convInfo.outHeight * convInfo.outWidth :\n                                       convInfo.outChannels;\n    const dimBOuter = isChannelsLast ? convInfo.outChannels :\n                                       convInfo.outHeight * convInfo.outWidth;\n    const dimInner =\n        convInfo.filterHeight * convInfo.filterWidth * convInfo.inChannels;\n    dimensions.push(\n        {type: 'int32', data: [dimAOuter]}, {type: 'int32', data: [dimBOuter]},\n        {type: 'int32', data: [dimInner]});\n\n    // Experiments show that sequential access is more friendly for Intel GPUs.\n    const sequentialAccessByThreads = backend.adapterInfo.isIntel();\n    program = new Conv2DMMProgram(\n        convInfo, dimAOuter, dimBOuter, dimInner, hasBias, activation,\n        hasPreluActivationWeights, sequentialAccessByThreads);\n  }\n\n  const intermediates: TensorInfo[] = [];\n  const inputVar: TensorInfo[] = [x, filter];\n  if (hasBias) {\n    if (!isChannelsLast && bias.shape.length === 1) {\n      bias = reshape(\n          {inputs: {x: bias}, backend, attrs: {shape: [bias.shape[0], 1, 1]}});\n      intermediates.push(bias);\n    }\n    inputVar.push(bias);\n  }\n  if (hasPreluActivationWeights) {\n    if (!isChannelsLast && preluActivationWeights.shape.length === 1) {\n      preluActivationWeights = reshape({\n        inputs: {x: preluActivationWeights},\n        backend,\n        attrs: {shape: [preluActivationWeights.shape[0], 1, 1]}\n      });\n      intermediates.push(preluActivationWeights);\n    }\n    inputVar.push(preluActivationWeights);\n  }\n  if (activation === 'leakyrelu') {\n    dimensions.push({type: 'float32', data: [leakyreluAlpha]});\n    program.uniforms += ' alpha : f32,';\n  }\n  const out = backend.runWebGPUProgram(program, inputVar, x.dtype, dimensions);\n  for (const i of intermediates) {\n    backend.disposeData(i.dataId);\n  }\n  return out;\n}\n"],"mappings":"AAAA;;;;;;;;;;;;;;;;AAiBA,SAAsBA,GAAG,QAAmB,uBAAuB;AAGnE,SAAQC,eAAe,QAAO,qBAAqB;AACnD,SAAQC,kBAAkB,QAAO,wBAAwB;AACzD,SAAQC,aAAa,QAAO,kBAAkB;AAG9C,SAAQC,eAAe,QAAO,oBAAoB;AAClD,SAAQC,OAAO,QAAO,WAAW;AAajC;AACA;AACA;AACA;AACA;AACA;AACA,SAASC,sBAAsBA,CAC3BC,KAAe,EAAEC,cAAuB;EAC1C,MAAMC,MAAM,GAAGF,KAAK,CAACE,MAAM;EAC3B,IAAIA,MAAM,IAAI,CAAC,EAAE;IACf,OAAOD,cAAc,GACjB,CACE,GAAGD,KAAK,CAACG,KAAK,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC,aACtBH,KAAK,CAACE,MAAM,GAAG,CAAC,CAAC,GAAGF,KAAK,CAACE,MAAM,GAAG,CAAC,CAAC,CAAC,sBACtCF,KAAK,CAACE,MAAM,GAAG,CAAC,CAAC,CAAC,cACnB,GACD,CACE,GAAGF,KAAK,CAACG,KAAK,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC,aAAaH,KAAK,CAACE,MAAM,GAAG,CAAC,CAAC,CAAC,eACrDF,KAAK,CAACE,MAAM,GAAG,CAAC,CAAC,GAAGF,KAAK,CAACE,MAAM,GAAG,CAAC,CAAC,CAAC,qBACvC;GACN,MAAM,IAAI,CAACD,cAAc,IAAIC,MAAM,KAAK,CAAC,IAAIF,KAAK,CAAC,CAAC,CAAC,GAAG,CAAC,EAAE;IAC1D,OAAO,CAACA,KAAK,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC;GACrB,MAAM;IACL,OAAO,IAAI;;AAEf;AAEA;AACA;AACA;AACA,SAASI,cAAcA,CAAC;EACtBC,CAAC;EACDC,MAAM;EACNC,QAAQ;EACRC,OAAO;EACPC,IAAI,GAAG,IAAI;EACXC,sBAAsB,GAAG,IAAI;EAC7BC,cAAc,GAAG,CAAC;EAClBC,UAAU,GAAG;AAAI,CACJ;EACb,MAAMX,cAAc,GAAGM,QAAQ,CAACM,UAAU,KAAK,cAAc;EAC7D,MAAMC,UAAU,GAAGb,cAAc,GAAG,KAAK,GAAG,IAAI;EAChD,MAAMc,UAAU,GAAG,KAAK;EAExB,MAAMC,QAAQ,GAAGf,cAAc,IAC3BM,QAAQ,CAACU,YAAY,KAAKV,QAAQ,CAACW,QAAQ,IAC3CX,QAAQ,CAACY,WAAW,KAAKZ,QAAQ,CAACa,OAAO,IACzCb,QAAQ,CAACc,OAAO,CAACC,IAAI,KAAK,OAAO;EACrC,MAAMC,aAAa,GAAiB,EAAE;EACtC,IAAIC,SAAS;EACb,IAAIC,cAAc;EAElB,IAAIT,QAAQ,EAAE;IACZ,MAAMU,SAAS,GACXnB,QAAQ,CAACW,QAAQ,GAAGX,QAAQ,CAACa,OAAO,GAAGb,QAAQ,CAACoB,UAAU;IAC9DH,SAAS,GAAG1B,OAAO,CAAC;MAClB8B,MAAM,EAAE;QAACvB;MAAC,CAAC;MACXG,OAAO;MACPqB,KAAK,EAAE;QAAC7B,KAAK,EAAE,CAAC,CAAC,EAAEO,QAAQ,CAACuB,SAAS,EAAEJ,SAAS;MAAC;KAClD,CAAC;IACFD,cAAc,GAAG3B,OAAO,CAAC;MACvB8B,MAAM,EAAE;QAACvB,CAAC,EAAEC;MAAM,CAAC;MACnBE,OAAO;MACPqB,KAAK,EAAE;QAAC7B,KAAK,EAAE,CAAC,CAAC,EAAE0B,SAAS,EAAEnB,QAAQ,CAACwB,WAAW;MAAC;KACpD,CAAC;GACH,MAAM;IACLP,SAAS,GAAG1B,OAAO,CAAC;MAClB8B,MAAM,EAAE;QAACvB;MAAC,CAAC;MACXG,OAAO;MACPqB,KAAK,EAAE;QACL7B,KAAK,EAAEC,cAAc,GACjB,CACEM,QAAQ,CAACuB,SAAS,EAAEvB,QAAQ,CAACW,QAAQ,GAAGX,QAAQ,CAACa,OAAO,EACxDb,QAAQ,CAACoB,UAAU,CACpB,GACD,CACEpB,QAAQ,CAACuB,SAAS,EAAEvB,QAAQ,CAACoB,UAAU,EACvCpB,QAAQ,CAACW,QAAQ,GAAGX,QAAQ,CAACa,OAAO;;KAG7C,CAAC;IACFK,cAAc,GAAG3B,OAAO,CAAC;MACvB8B,MAAM,EAAE;QAACvB,CAAC,EAAEC;MAAM,CAAC;MACnBE,OAAO;MACPqB,KAAK,EAAE;QAAC7B,KAAK,EAAE,CAAC,CAAC,EAAEO,QAAQ,CAACoB,UAAU,EAAEpB,QAAQ,CAACwB,WAAW;MAAC;KAC9D,CAAC;;EAEJR,aAAa,CAACS,IAAI,CAACR,SAAS,CAAC;EAC7BD,aAAa,CAACS,IAAI,CAACP,cAAc,CAAC;EAElC,IAAIf,sBAAsB,IAAI,IAAI,EAAE;IAClC,MAAMuB,WAAW,GACblC,sBAAsB,CAACW,sBAAsB,CAACV,KAAK,EAAEC,cAAc,CAAC;IACxE,IAAIgC,WAAW,IAAI,IAAI,EAAE;MACvBvB,sBAAsB,GAAGZ,OAAO,CAAC;QAC/B8B,MAAM,EAAE;UAACvB,CAAC,EAAEK;QAAsB,CAAC;QACnCF,OAAO;QACPqB,KAAK,EAAE;UAAC7B,KAAK,EAAEiC;QAAW;OAC3B,CAAC;MACFV,aAAa,CAACS,IAAI,CAACtB,sBAAsB,CAAC;;;EAI9C,IAAID,IAAI,IAAI,IAAI,EAAE;IAChB,MAAMwB,WAAW,GAAGlC,sBAAsB,CAACU,IAAI,CAACT,KAAK,EAAEC,cAAc,CAAC;IACtE,IAAIgC,WAAW,IAAI,IAAI,EAAE;MACvBxB,IAAI,GAAGX,OAAO,CAAC;QAAC8B,MAAM,EAAE;UAACvB,CAAC,EAAEI;QAAI,CAAC;QAAED,OAAO;QAAEqB,KAAK,EAAE;UAAC7B,KAAK,EAAEiC;QAAW;MAAC,CAAC,CAAC;MACzEV,aAAa,CAACS,IAAI,CAACvB,IAAI,CAAC;;;EAI5B,MAAMyB,MAAM,GAAGrC,eAAe,CAAC;IAC7BsC,CAAC,EAAElC,cAAc,GAAGuB,SAAS,GAAGC,cAAc;IAC9CW,CAAC,EAAEnC,cAAc,GAAGwB,cAAc,GAAGD,SAAS;IAC9CV,UAAU;IACVC,UAAU;IACVP,OAAO;IACPC,IAAI;IACJG,UAAU;IACVF,sBAAsB;IACtBC;GACD,CAAC;EACF,MAAM0B,GAAG,GAAGvC,OAAO,CACf;IAAC8B,MAAM,EAAE;MAACvB,CAAC,EAAE6B;IAAM,CAAC;IAAE1B,OAAO;IAAEqB,KAAK,EAAE;MAAC7B,KAAK,EAAEO,QAAQ,CAAC+B;IAAQ;EAAC,CAAC,CAAC;EACtEf,aAAa,CAACS,IAAI,CAACE,MAAM,CAAC;EAE1B,KAAK,MAAMK,CAAC,IAAIhB,aAAa,EAAE;IAC7Bf,OAAO,CAACgC,WAAW,CAACD,CAAC,CAACE,MAAM,CAAC;;EAG/B,OAAOJ,GAAG;AACZ;AAEA;AACA;AACA,SAASK,gBAAgBA,CAAC;EACxBrC,CAAC;EACDC,MAAM;EACNC,QAAQ;EACRC,OAAO;EACPC,IAAI,GAAG,IAAI;EACXC,sBAAsB,GAAG,IAAI;EAC7BC,cAAc,GAAG,CAAC;EAClBC,UAAU,GAAG;AAAI,CACJ;EACb;EACA;EACA;EACA;EACA;EACA;EACA,MAAM;IACJO,WAAW;IACXF,YAAY;IACZU,UAAU;IACVgB,WAAW;IACXC,YAAY;IACZvB,OAAO;IACPwB,QAAQ;IACRC,SAAS;IACTC,aAAa;IACbC,cAAc;IACdnC;EAAU,CACX,GAAGN,QAAQ;EAEZ,MAAMN,cAAc,GAAGY,UAAU,KAAK,cAAc;EAEpD,MAAMa,SAAS,GAAGP,WAAW,GAAGF,YAAY,GAAGU,UAAU;EACzD,MAAMsB,OAAO,GAAGH,SAAS,GAAGD,QAAQ;EACpC,MAAMK,UAAU,GAAGjD,cAAc,GAAG,CAACM,QAAQ,CAACuB,SAAS,EAAEmB,OAAO,EAAEvB,SAAS,CAAC,GACxC,CAACnB,QAAQ,CAACuB,SAAS,EAAEJ,SAAS,EAAEuB,OAAO,CAAC;EAE5E,MAAME,aAAa,GAAG,IAAIvD,aAAa,CAACsD,UAAU,EAAEjD,cAAc,CAAC;EACnE,MAAMmD,UAAU,GAAG,CACjB;IAAC9B,IAAI,EAAE,OAAO;IAAE+B,IAAI,EAAE,CAAChC,OAAO,CAACiC,GAAG,EAAEjC,OAAO,CAACkC,IAAI;EAAC,CAAC,EAClD;IAACjC,IAAI,EAAE,OAAO;IAAE+B,IAAI,EAAE,CAACT,YAAY,EAAED,WAAW;EAAC,CAAC,EAClD;IAACrB,IAAI,EAAE,OAAO;IAAE+B,IAAI,EAAE,CAACL,cAAc,EAAED,aAAa;EAAC,CAAC,EACtD;IAACzB,IAAI,EAAE,OAAO;IAAE+B,IAAI,EAAE,CAACR,QAAQ;EAAC,CAAC,EACjC;IAACvB,IAAI,EAAE,OAAO;IAAE+B,IAAI,EAAE,CAAC1B,UAAU,GAAGR,WAAW;EAAC,CAAC,EACjD;IAACG,IAAI,EAAE,OAAO;IAAE+B,IAAI,EAAE,CAAC1B,UAAU;EAAC,CAAC,CACpC;EACD,MAAM6B,KAAK,GACPhD,OAAO,CAACiD,gBAAgB,CAACN,aAAa,EAAE,CAAC9C,CAAC,CAAC,EAAEA,CAAC,CAACqD,KAAK,EAAEN,UAAU,CAAC;EAErE,MAAM7B,aAAa,GAAiB,EAAE;EACtCA,aAAa,CAACS,IAAI,CAACwB,KAAK,CAAC;EAEzB,MAAM/B,cAAc,GAAG3B,OAAO,CAC1B;IAAC8B,MAAM,EAAE;MAACvB,CAAC,EAAEC;IAAM,CAAC;IAAEE,OAAO;IAAEqB,KAAK,EAAE;MAAC7B,KAAK,EAAE,CAAC,CAAC,EAAE0B,SAAS,EAAE,CAAC,CAAC;IAAC;EAAC,CAAC,CAAC;EACvEH,aAAa,CAACS,IAAI,CAACP,cAAc,CAAC;EAElC,IAAIf,sBAAsB,IAAI,IAAI,EAAE;IAClC,MAAMuB,WAAW,GACblC,sBAAsB,CAACW,sBAAsB,CAACV,KAAK,EAAEC,cAAc,CAAC;IACxE,IAAIgC,WAAW,IAAI,IAAI,EAAE;MACvBvB,sBAAsB,GAAGZ,OAAO,CAAC;QAC/B8B,MAAM,EAAE;UAACvB,CAAC,EAAEK;QAAsB,CAAC;QACnCF,OAAO;QACPqB,KAAK,EAAE;UAAC7B,KAAK,EAAEiC;QAAW;OAC3B,CAAC;MACFV,aAAa,CAACS,IAAI,CAACtB,sBAAsB,CAAC;;;EAI9C,IAAID,IAAI,IAAI,IAAI,EAAE;IAChB,MAAMwB,WAAW,GAAGlC,sBAAsB,CAACU,IAAI,CAACT,KAAK,EAAEC,cAAc,CAAC;IACtE,IAAIgC,WAAW,IAAI,IAAI,EAAE;MACvBxB,IAAI,GAAGX,OAAO,CAAC;QAAC8B,MAAM,EAAE;UAACvB,CAAC,EAAEI;QAAI,CAAC;QAAED,OAAO;QAAEqB,KAAK,EAAE;UAAC7B,KAAK,EAAEiC;QAAW;MAAC,CAAC,CAAC;MACzEV,aAAa,CAACS,IAAI,CAACvB,IAAI,CAAC;;;EAI5B,MAAMK,UAAU,GAAGb,cAAc,GAAG,KAAK,GAAG,IAAI;EAChD,MAAMc,UAAU,GAAG,KAAK;EACxB,MAAMmB,MAAM,GAAGrC,eAAe,CAAC;IAC7BsC,CAAC,EAAElC,cAAc,GAAGuD,KAAK,GAAG/B,cAAc;IAC1CW,CAAC,EAAEnC,cAAc,GAAGwB,cAAc,GAAG+B,KAAK;IAC1C1C,UAAU;IACVC,UAAU;IACVP,OAAO;IACPC,IAAI;IACJG,UAAU;IACVF,sBAAsB;IACtBC;GACD,CAAC;EACF,MAAM0B,GAAG,GAAGvC,OAAO,CACf;IAAC8B,MAAM,EAAE;MAACvB,CAAC,EAAE6B;IAAM,CAAC;IAAE1B,OAAO;IAAEqB,KAAK,EAAE;MAAC7B,KAAK,EAAEO,QAAQ,CAAC+B;IAAQ;EAAC,CAAC,CAAC;EACtEf,aAAa,CAACS,IAAI,CAACE,MAAM,CAAC;EAC1B,KAAK,MAAMK,CAAC,IAAIhB,aAAa,EAAE;IAC7Bf,OAAO,CAACgC,WAAW,CAACD,CAAC,CAACE,MAAM,CAAC;;EAG/B,OAAOJ,GAAG;AACZ;AAEA,OAAM,SAAUsB,UAAUA,CAAC;EACzBtD,CAAC;EACDC,MAAM;EACNC,QAAQ;EACRC,OAAO;EACPC,IAAI,GAAG,IAAI;EACXC,sBAAsB,GAAG,IAAI;EAC7BC,cAAc,GAAG,CAAC;EAClBC,UAAU,GAAG;AAAI,CACJ;EACb,MAAMgD,OAAO,GAAGnD,IAAI,IAAI,IAAI;EAC5B,MAAMoD,yBAAyB,GAAGnD,sBAAsB,IAAI,IAAI;EAChE,MAAMT,cAAc,GAAGM,QAAQ,CAACM,UAAU,KAAK,cAAc;EAC7D,MAAMG,QAAQ,GAAGf,cAAc,IAC3BM,QAAQ,CAACU,YAAY,KAAKV,QAAQ,CAACW,QAAQ,IAC3CX,QAAQ,CAACY,WAAW,KAAKZ,QAAQ,CAACa,OAAO,IACzCb,QAAQ,CAACc,OAAO,CAACC,IAAI,KAAK,OAAO;EACrC,MAAMwC,cAAc,GAAGrE,GAAG,EAAE,CAACsE,OAAO,CAAC,+BAA+B,CAAC;EAErE,IAAI,CAACD,cAAc,KACd9C,QAAQ,IACPT,QAAQ,CAACU,YAAY,KAAK,CAAC,IAAIV,QAAQ,CAACY,WAAW,KAAK,CAAC,IACzDZ,QAAQ,CAACyC,cAAc,KAAK,CAAC,IAAIzC,QAAQ,CAACwC,aAAa,KAAK,CAAC,IAC7DxC,QAAQ,CAACqC,YAAY,KAAK,CAAC,IAAIrC,QAAQ,CAACoC,WAAW,KAAK,CAAC,KACxDpC,QAAQ,CAACc,OAAO,CAACC,IAAI,KAAK,MAAM,IAChCf,QAAQ,CAACc,OAAO,CAACC,IAAI,KAAK,OAAO,CAAE,CAAC,EAAE;IAC3C,OAAOlB,cAAc,CAAC;MACpBC,CAAC;MACDC,MAAM;MACNC,QAAQ;MACRC,OAAO;MACPC,IAAI;MACJG,UAAU;MACVF,sBAAsB;MACtBC;KACD,CAAC;;EAGJ,MAAMqD,kBAAkB,GACpBvE,GAAG,EAAE,CAACwE,SAAS,CAAC,oDAAoD,CAAC;EACzE,MAAMC,6BAA6B,GAAGF,kBAAkB,GAAG,CAAC,CAAC,GACzDA,kBAAkB,GAClBxD,OAAO,CAAC0D,6BAA6B;EACzC,MAAMC,iBAAiB,GAAG5D,QAAQ,CAACuB,SAAS,GACxCsC,IAAI,CAACC,IAAI,CAAE9D,QAAQ,CAACuC,SAAS,GAAGvC,QAAQ,CAACsC,QAAQ,GAAI,EAAE,CAAC,GACxDuB,IAAI,CAACC,IAAI,CAAC9D,QAAQ,CAACwB,WAAW,GAAG,EAAE,CAAC;EACxC,IAAItC,GAAG,EAAE,CAACsE,OAAO,CAAC,oCAAoC,CAAC,IACnDI,iBAAiB,IAAID,6BAA6B,EAAE;IACtD,OAAOxB,gBAAgB,CAAC;MACtBrC,CAAC;MACDC,MAAM;MACNC,QAAQ;MACRC,OAAO;MACPC,IAAI;MACJC,sBAAsB;MACtBC,cAAc;MACdC;KACD,CAAC;;EAGJ,IAAI0D,OAAsB;EAC1B,MAAMjD,OAAO,GAAG,CAACd,QAAQ,CAACc,OAAO,CAACiC,GAAG,EAAE/C,QAAQ,CAACc,OAAO,CAACkC,IAAI,CAAC;EAC7D,MAAMH,UAAU,GAAG,CACjB;IAAC9B,IAAI,EAAE,OAAO;IAAE+B,IAAI,EAAE,CAAC9C,QAAQ,CAACU,YAAY,EAAEV,QAAQ,CAACY,WAAW;EAAC,CAAC,EACpE;IAACG,IAAI,EAAE,OAAO;IAAE+B,IAAI,EAAE,CAAC,GAAGhC,OAAO;EAAC,CAAC,EACnC;IAACC,IAAI,EAAE,OAAO;IAAE+B,IAAI,EAAE,CAAC9C,QAAQ,CAACqC,YAAY,EAAErC,QAAQ,CAACoC,WAAW;EAAC,CAAC,EACpE;IAACrB,IAAI,EAAE,OAAO;IAAE+B,IAAI,EAAE,CAAC9C,QAAQ,CAACyC,cAAc,EAAEzC,QAAQ,CAACwC,aAAa;EAAC,CAAC,CACzE;EACD,IAAIe,cAAc,EAAE;IAClBQ,OAAO,GAAG,IAAI3E,kBAAkB,CAC5BY,QAAQ,EAAEqD,OAAO,EAAEhD,UAAU,EAAEiD,yBAAyB,CAAC;GAC9D,MAAM;IACL,MAAMU,SAAS,GAAGtE,cAAc,GAAGM,QAAQ,CAACuC,SAAS,GAAGvC,QAAQ,CAACsC,QAAQ,GACtCtC,QAAQ,CAACwB,WAAW;IACvD,MAAMyC,SAAS,GAAGvE,cAAc,GAAGM,QAAQ,CAACwB,WAAW,GACpBxB,QAAQ,CAACuC,SAAS,GAAGvC,QAAQ,CAACsC,QAAQ;IACzE,MAAM4B,QAAQ,GACVlE,QAAQ,CAACU,YAAY,GAAGV,QAAQ,CAACY,WAAW,GAAGZ,QAAQ,CAACoB,UAAU;IACtEyB,UAAU,CAACpB,IAAI,CACX;MAACV,IAAI,EAAE,OAAO;MAAE+B,IAAI,EAAE,CAACkB,SAAS;IAAC,CAAC,EAAE;MAACjD,IAAI,EAAE,OAAO;MAAE+B,IAAI,EAAE,CAACmB,SAAS;IAAC,CAAC,EACtE;MAAClD,IAAI,EAAE,OAAO;MAAE+B,IAAI,EAAE,CAACoB,QAAQ;IAAC,CAAC,CAAC;IAEtC;IACA,MAAMC,yBAAyB,GAAGlE,OAAO,CAACmE,WAAW,CAACC,OAAO,EAAE;IAC/DN,OAAO,GAAG,IAAI5E,eAAe,CACzBa,QAAQ,EAAEgE,SAAS,EAAEC,SAAS,EAAEC,QAAQ,EAAEb,OAAO,EAAEhD,UAAU,EAC7DiD,yBAAyB,EAAEa,yBAAyB,CAAC;;EAG3D,MAAMnD,aAAa,GAAiB,EAAE;EACtC,MAAMsD,QAAQ,GAAiB,CAACxE,CAAC,EAAEC,MAAM,CAAC;EAC1C,IAAIsD,OAAO,EAAE;IACX,IAAI,CAAC3D,cAAc,IAAIQ,IAAI,CAACT,KAAK,CAACE,MAAM,KAAK,CAAC,EAAE;MAC9CO,IAAI,GAAGX,OAAO,CACV;QAAC8B,MAAM,EAAE;UAACvB,CAAC,EAAEI;QAAI,CAAC;QAAED,OAAO;QAAEqB,KAAK,EAAE;UAAC7B,KAAK,EAAE,CAACS,IAAI,CAACT,KAAK,CAAC,CAAC,CAAC,EAAE,CAAC,EAAE,CAAC;QAAC;MAAC,CAAC,CAAC;MACxEuB,aAAa,CAACS,IAAI,CAACvB,IAAI,CAAC;;IAE1BoE,QAAQ,CAAC7C,IAAI,CAACvB,IAAI,CAAC;;EAErB,IAAIoD,yBAAyB,EAAE;IAC7B,IAAI,CAAC5D,cAAc,IAAIS,sBAAsB,CAACV,KAAK,CAACE,MAAM,KAAK,CAAC,EAAE;MAChEQ,sBAAsB,GAAGZ,OAAO,CAAC;QAC/B8B,MAAM,EAAE;UAACvB,CAAC,EAAEK;QAAsB,CAAC;QACnCF,OAAO;QACPqB,KAAK,EAAE;UAAC7B,KAAK,EAAE,CAACU,sBAAsB,CAACV,KAAK,CAAC,CAAC,CAAC,EAAE,CAAC,EAAE,CAAC;QAAC;OACvD,CAAC;MACFuB,aAAa,CAACS,IAAI,CAACtB,sBAAsB,CAAC;;IAE5CmE,QAAQ,CAAC7C,IAAI,CAACtB,sBAAsB,CAAC;;EAEvC,IAAIE,UAAU,KAAK,WAAW,EAAE;IAC9BwC,UAAU,CAACpB,IAAI,CAAC;MAACV,IAAI,EAAE,SAAS;MAAE+B,IAAI,EAAE,CAAC1C,cAAc;IAAC,CAAC,CAAC;IAC1D2D,OAAO,CAACQ,QAAQ,IAAI,eAAe;;EAErC,MAAMzC,GAAG,GAAG7B,OAAO,CAACiD,gBAAgB,CAACa,OAAO,EAAEO,QAAQ,EAAExE,CAAC,CAACqD,KAAK,EAAEN,UAAU,CAAC;EAC5E,KAAK,MAAMb,CAAC,IAAIhB,aAAa,EAAE;IAC7Bf,OAAO,CAACgC,WAAW,CAACD,CAAC,CAACE,MAAM,CAAC;;EAE/B,OAAOJ,GAAG;AACZ","ignoreList":[]},"metadata":{},"sourceType":"module","externalDependencies":[]}